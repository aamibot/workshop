{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation with Amazon a SageMaker Processing Job and Apache Spark\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Apache Spark are used to pre-process data sets in order to prepare them for training. In this notebook we'll use Amazon SageMaker Processing, and leverage the power of Apache Spark in a managed SageMaker environment to run our processing workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/prepare_dataset_bert.png)\n",
    "\n",
    "![](img/processing.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment\n",
    "\n",
    "Let's start by specifying:\n",
    "* The S3 bucket and prefixes that you use for training and model data. Use the default bucket specified by the Amazon SageMaker session.\n",
    "* The IAM role ARN used to give processing and training access to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-140773038493/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "s3_input_data = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(s3_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-25 17:24:19   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\r\n",
      "2020-07-25 17:24:22   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Spark Docker Image to Run the Processing Job\n",
    "\n",
    "An example Spark container is included in the `./container` directory of this example. The container handles the bootstrapping of all Spark configuration, and serves as a wrapper around the `spark-submit` CLI. At a high level the container provides:\n",
    "* A set of default Spark/YARN/Hadoop configurations\n",
    "* A bootstrapping script for configuring and starting up Spark master/worker nodes\n",
    "* A wrapper around the `spark-submit` CLI to submit a Spark application\n",
    "\n",
    "\n",
    "After the container build and push process is complete, use the Amazon SageMaker Python SDK to submit a managed, distributed Spark application that performs our dataset processing.\n",
    "\n",
    "Build the example Spark container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = 'amazon-reviews-spark-processor'\n",
    "docker_tag = 'latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  4.441MB\n",
      "Step 1/37 : FROM openjdk:8-jre-slim\n",
      " ---> f2e91f81bf2c\n",
      "Step 2/37 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 62231d826b5c\n",
      "Step 3/37 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> bce0d33778c9\n",
      "Step 4/37 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Using cache\n",
      " ---> aefbffba7dcd\n",
      "Step 5/37 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> b2df2b55de1c\n",
      "Step 6/37 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 89a06781e68c\n",
      "Step 7/37 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> e4cfc81c854b\n",
      "Step 8/37 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> 6fb94f0f28c4\n",
      "Step 9/37 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> 39444b1d9efa\n",
      "Step 10/37 : ENV HADOOP_VERSION 3.2.1\n",
      " ---> Using cache\n",
      " ---> 0d511b3986f9\n",
      "Step 11/37 : ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION\n",
      " ---> Using cache\n",
      " ---> 1117bac7ad00\n",
      "Step 12/37 : ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
      " ---> Using cache\n",
      " ---> da23292989cf\n",
      "Step 13/37 : ENV PATH $PATH:$HADOOP_HOME/bin\n",
      " ---> Using cache\n",
      " ---> d62f50ea9f6a\n",
      "Step 14/37 : RUN curl -sL --retry 3   \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\"   | gunzip   | tar -x -C /usr/  && rm -rf $HADOOP_HOME/share/doc  && chown -R root:root $HADOOP_HOME\n",
      " ---> Using cache\n",
      " ---> a04dd2f46fc7\n",
      "Step 15/37 : ENV SPARK_VERSION 2.4.6\n",
      " ---> Using cache\n",
      " ---> 46e9bbcdc8e7\n",
      "Step 16/37 : ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop\n",
      " ---> Using cache\n",
      " ---> a4e4e7caddec\n",
      "Step 17/37 : ENV SPARK_HOME /usr/spark-${SPARK_VERSION}\n",
      " ---> Using cache\n",
      " ---> a5ba2af2f18a\n",
      "Step 18/37 : ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*\"\n",
      " ---> Using cache\n",
      " ---> 8ea02d359999\n",
      "Step 19/37 : ENV PATH $PATH:${SPARK_HOME}/bin\n",
      " ---> Using cache\n",
      " ---> c70620e6048f\n",
      "Step 20/37 : RUN curl -sL --retry 3   \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz\"   | gunzip   | tar x -C /usr/  && mv /usr/$SPARK_PACKAGE $SPARK_HOME  && chown -R root:root $SPARK_HOME\n",
      " ---> Using cache\n",
      " ---> 137b3cc39006\n",
      "Step 21/37 : ENV PYSPARK_PYTHON=/usr/bin/python3\n",
      " ---> Using cache\n",
      " ---> b47d9dfebd3a\n",
      "Step 22/37 : ENV PATH=\"/usr/bin:/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> bc57c87c711c\n",
      "Step 23/37 : ENV YARN_RESOURCEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> dbaeb44b6278\n",
      "Step 24/37 : ENV YARN_NODEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> ba734583983b\n",
      "Step 25/37 : ENV HDFS_NAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 43551c462b58\n",
      "Step 26/37 : ENV HDFS_DATANODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 7e2d0310b896\n",
      "Step 27/37 : ENV HDFS_SECONDARYNAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> ebbc5b36d5e6\n",
      "Step 28/37 : COPY program /opt/program\n",
      " ---> 0f9aa8408462\n",
      "Step 29/37 : RUN chmod +x /opt/program/submit\n",
      " ---> Running in 13bf191daf95\n",
      "Removing intermediate container 13bf191daf95\n",
      " ---> 0bb01e902f24\n",
      "Step 30/37 : COPY hadoop-config /opt/hadoop-config\n",
      " ---> e456e4d5fc80\n",
      "Step 31/37 : COPY jars /usr/jars\n",
      " ---> ffbad3a934ba\n",
      "Step 32/37 : WORKDIR $SPARK_HOME\n",
      " ---> Running in d15616e43be2\n",
      "Removing intermediate container d15616e43be2\n",
      " ---> 1dffd68cbdd3\n",
      "Step 33/37 : RUN pip3 install -q pip --upgrade\n",
      " ---> Running in 742cea11006e\n",
      "Removing intermediate container 742cea11006e\n",
      " ---> 2c2e580bf5cb\n",
      "Step 34/37 : RUN pip3 install -q wrapt --upgrade --ignore-installed\n",
      " ---> Running in fadc209b24d4\n",
      "\u001b[91mWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[0mRemoving intermediate container fadc209b24d4\n",
      " ---> 559b44e2590a\n",
      "Step 35/37 : RUN pip3 install -q transformers==2.8.0\n",
      " ---> Running in 27e56b82ef7d\n",
      "\u001b[91mWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[0mRemoving intermediate container 27e56b82ef7d\n",
      " ---> 71b4aadeb95d\n",
      "Step 36/37 : RUN pip3 install -q tensorflow==2.1.0 --upgrade --ignore-installed\n",
      " ---> Running in d99bb6733aee\n",
      "\u001b[91mWARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "\u001b[0mRemoving intermediate container d99bb6733aee\n",
      " ---> 8aa76fe478a8\n",
      "Step 37/37 : ENTRYPOINT [\"/opt/program/submit\"]\n",
      " ---> Running in 82494da056f5\n",
      "Removing intermediate container 82494da056f5\n",
      " ---> 64dded681af3\n",
      "Successfully built 64dded681af3\n",
      "Successfully tagged amazon-reviews-spark-processor:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $docker_repo:$docker_tag -f container/Dockerfile ./container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Amazon Elastic Container Registry (Amazon ECR) repository for the Spark container and push the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140773038493.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-processor:latest\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "image_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account_id, region, docker_repo, docker_tag)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ECR repository and push docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore any `RepositoryNotFoundException` error, we are creating the repo right after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (RepositoryNotFoundException) when calling the DescribeRepositories operation: The repository with name 'amazon-reviews-spark-processor' does not exist in the registry with id '140773038493'\n",
      "{\n",
      "    \"repository\": {\n",
      "        \"repositoryArn\": \"arn:aws:ecr:us-west-2:140773038493:repository/amazon-reviews-spark-processor\",\n",
      "        \"registryId\": \"140773038493\",\n",
      "        \"repositoryName\": \"amazon-reviews-spark-processor\",\n",
      "        \"repositoryUri\": \"140773038493.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-processor\",\n",
      "        \"createdAt\": 1595702488.0,\n",
      "        \"imageTagMutability\": \"MUTABLE\",\n",
      "        \"imageScanningConfiguration\": {\n",
      "            \"scanOnPush\": false\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws ecr describe-repositories --repository-names $docker_repo || aws ecr create-repository --repository-name $docker_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag $docker_repo:$docker_tag $image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [140773038493.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-processor]\n",
      "\n",
      "\u001b[1Bdcf6e5d7: Preparing \n",
      "\u001b[1B2184851c: Preparing \n",
      "\u001b[1Bb4dce864: Preparing \n",
      "\u001b[1Bd67bce84: Preparing \n",
      "\u001b[1B0c69a1da: Preparing \n",
      "\u001b[1B945badba: Preparing \n",
      "\u001b[1Bf6318e65: Preparing \n",
      "\u001b[1B68ba4e24: Preparing \n",
      "\u001b[1B5db8da73: Preparing \n",
      "\u001b[1B484ea491: Preparing \n",
      "\u001b[1B882873f1: Preparing \n",
      "\u001b[1Bef63f751: Preparing \n",
      "\u001b[1B09cb497b: Preparing \n",
      "\u001b[6B5db8da73: Waiting g \n",
      "\u001b[1B4809f1d0: Preparing \n",
      "\u001b[1Bc95dcfbb: Preparing \n",
      "\u001b[1B38d128fe: Preparing \n",
      "\u001b[5Be96fc851: Waiting g \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[19Bcf6e5d7: Pushing  1.794GB/2.011GB\u001b[18A\u001b[2K\u001b[17A\u001b[2K\u001b[16A\u001b[2K\u001b[15A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[16A\u001b[2K\u001b[14A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[13A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[12A\u001b[2K\u001b[13A\u001b[2K\u001b[18A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[9A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[8A\u001b[2K\u001b[18A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[18A\u001b[2K\u001b[10A\u001b[2K\u001b[18A\u001b[2K\u001b[11A\u001b[2K\u001b[18A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[18A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[18A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[18A\u001b[2K\u001b[10A\u001b[2K\u001b[18A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[10A\u001b[2K\u001b[18A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[7A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[19A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[5A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[1A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[19Bcf6e5d7: Pushed   2.021GB/2.011GB\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2K\u001b[19A\u001b[2Klatest: digest: sha256:3335980bdfcab313fe973147d70f4bd0994355faa82a378e5f26b336b07b4e71 size: 4318\n"
     ]
    }
   ],
   "source": [
    "!docker push $image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Job using Amazon SageMaker Processing Jobs\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the Spark container that was just built, and a Spark ML script for processing in the job configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the Spark processing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m unicode_literals\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'pip', '--upgrade'])\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'wrapt', '--upgrade', '--ignore-installed'])\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0', '--ignore-installed'])\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[36mprint\u001b[39;49;00m(tf.__version__)\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SparkSession\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mml\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Pipeline\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mml\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlinalg\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DenseVector\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m split\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m udf, col\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtypes\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\r\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m# We set sequences to be at most 128 tokens long.\u001b[39;49;00m\r\n",
      "MAX_SEQ_LENGTH = \u001b[34m128\u001b[39;49;00m\r\n",
      "DATA_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "LABEL_COLUMN = \u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "LABEL_VALUES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\r\n",
      "\r\n",
      "label_map = {}\r\n",
      "\u001b[34mfor\u001b[39;49;00m (i, label) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(LABEL_VALUES):\r\n",
      "    label_map[label] = i\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInputFeatures\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"BERT feature vectors.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,\r\n",
      "               input_ids,\r\n",
      "               input_mask,\r\n",
      "               segment_ids,\r\n",
      "               label_id):\r\n",
      "    \u001b[36mself\u001b[39;49;00m.input_ids = input_ids\r\n",
      "    \u001b[36mself\u001b[39;49;00m.input_mask = input_mask\r\n",
      "    \u001b[36mself\u001b[39;49;00m.segment_ids = segment_ids\r\n",
      "    \u001b[36mself\u001b[39;49;00m.label_id = label_id\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mInput\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "  \u001b[33m\"\"\"A single training/test input for sequence classification.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "  \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, text, label=\u001b[34mNone\u001b[39;49;00m):\r\n",
      "    \u001b[33m\"\"\"Constructs an Input.\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m      text: string. The untokenized text of the first sequence. For single\u001b[39;49;00m\r\n",
      "\u001b[33m        sequence tasks, only this sequence must be specified.\u001b[39;49;00m\r\n",
      "\u001b[33m      label: (Optional) string. The label of the example. This should be\u001b[39;49;00m\r\n",
      "\u001b[33m        specified for train and dev examples, but not for test examples.\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[36mself\u001b[39;49;00m.text = text\r\n",
      "    \u001b[36mself\u001b[39;49;00m.label = label\r\n",
      "    \r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mconvert_input\u001b[39;49;00m(label, text):\r\n",
      "    \u001b[37m# First, we need to preprocess our data so that it matches the data BERT was trained on:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 1. Lowercase our text (if we're using a BERT lowercase model)\u001b[39;49;00m\r\n",
      "    \u001b[37m# 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m# 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# Fortunately, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m#    tokens = tokenizer.tokenize(text_input.text)\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Next, we need to do the following:\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# 4. Map our words to indexes using a vocab file that BERT provides\u001b[39;49;00m\r\n",
      "    \u001b[37m# 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\u001b[39;49;00m\r\n",
      "    \u001b[37m# 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    \u001b[37m# Again, the Transformers tokenizer does this for us!\u001b[39;49;00m\r\n",
      "    \u001b[37m#\u001b[39;49;00m\r\n",
      "    encode_plus_tokens = tokenizer.encode_plus(text,\r\n",
      "                                               pad_to_max_length=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                                               max_length=MAX_SEQ_LENGTH)\r\n",
      "\r\n",
      "    \u001b[37m# Convert the text-based tokens to ids from the pre-trained BERT vocabulary\u001b[39;49;00m\r\n",
      "    input_ids = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \u001b[37m# Specifies which tokens BERT should pay attention to (0 or 1)\u001b[39;49;00m\r\n",
      "    input_mask = encode_plus_tokens[\u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    \u001b[37m# Segment Ids are always 0 for single-sequence tasks (or 1 if two-sequence tasks)\u001b[39;49;00m\r\n",
      "    segment_ids = [\u001b[34m0\u001b[39;49;00m] * MAX_SEQ_LENGTH\r\n",
      "\r\n",
      "    \u001b[37m# Label for our training data (star_rating 1 through 5)\u001b[39;49;00m\r\n",
      "    label_id = label_map[label]\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: input_ids, \u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: input_mask, \u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: segment_ids, \u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [label_id]}\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mlist_arg\u001b[39;49;00m(raw_value):\r\n",
      "    \u001b[33m\"\"\"argparse type for a list of strings\"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(raw_value).split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\r\n",
      "    \u001b[37m# Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\u001b[39;49;00m\r\n",
      "    resconfig = {}\r\n",
      "    \u001b[34mtry\u001b[39;49;00m:\r\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m cfgfile:\r\n",
      "            resconfig = json.load(cfgfile)\r\n",
      "    \u001b[34mexcept\u001b[39;49;00m \u001b[36mFileNotFoundError\u001b[39;49;00m:\r\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mpass\u001b[39;49;00m \u001b[37m# Ignore\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# Local testing with CLI args\u001b[39;49;00m\r\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mProcess\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=list_arg,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, [\u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mComma-separated list of host names running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=resconfig.get(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m),\r\n",
      "        help=\u001b[33m'\u001b[39;49;00m\u001b[33mName of this host running the job\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--input-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/input/data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "        default=\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "    )\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransform\u001b[39;49;00m(spark, s3_input_data, s3_output_train_data, s3_output_validation_data, s3_output_test_data): \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mProcessing \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m => \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_input_data, s3_output_train_data, s3_output_validation_data, s3_output_test_data))\r\n",
      " \r\n",
      "    schema = StructType([\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mmarketplace\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mcustomer_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_id\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_parent\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_title\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mproduct_category\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mhelpful_votes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mtotal_votes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, IntegerType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mvine\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mverified_purchase\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_headline\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m),\r\n",
      "        StructField(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_date\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, StringType(), \u001b[34mTrue\u001b[39;49;00m)\r\n",
      "    ])\r\n",
      "    \r\n",
      "    df_csv = spark.read.csv(path=s3_input_data,\r\n",
      "                            sep=\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                            schema=schema,\r\n",
      "                            header=\u001b[34mTrue\u001b[39;49;00m,\r\n",
      "                            quote=\u001b[34mNone\u001b[39;49;00m)\r\n",
      "    df_csv.show()\r\n",
      "\r\n",
      "    \u001b[37m# This dataset should already be clean, but always good to double-check\u001b[39;49;00m\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShowing null review_body rows...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    df_csv.where(col(\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).isNull()).show()\r\n",
      "\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mShowing cleaned csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    df_csv_dropped = df_csv.na.drop(subset=[\u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    df_csv_dropped.show()\r\n",
      "\r\n",
      "    \u001b[37m# TODO:  Balance\u001b[39;49;00m\r\n",
      "    \r\n",
      "    features_df = df_csv_dropped.select([\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    features_df.show()\r\n",
      "\r\n",
      "    tfrecord_schema = StructType([\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m)),\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m)),\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m)),\r\n",
      "      StructField(\u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, ArrayType(IntegerType(), \u001b[34mFalse\u001b[39;49;00m))\r\n",
      "    ])\r\n",
      "\r\n",
      "    bert_transformer = udf(\u001b[34mlambda\u001b[39;49;00m text, label: convert_input(text, label), tfrecord_schema)\r\n",
      "\r\n",
      "    spark.udf.register(\u001b[33m'\u001b[39;49;00m\u001b[33mbert_transformer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, bert_transformer)\r\n",
      "\r\n",
      "    transformed_df = features_df.select(bert_transformer(\u001b[33m'\u001b[39;49;00m\u001b[33mstar_rating\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mreview_body\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).alias(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    transformed_df.show(truncate=\u001b[34mFalse\u001b[39;49;00m)\r\n",
      "\r\n",
      "    flattened_df = transformed_df.select(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords.*\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    flattened_df.show()\r\n",
      "\r\n",
      "    \u001b[37m# Split 90-5-5%\u001b[39;49;00m\r\n",
      "    train_df, validation_df, test_df = flattened_df.randomSplit([\u001b[34m0.9\u001b[39;49;00m, \u001b[34m0.05\u001b[39;49;00m, \u001b[34m0.05\u001b[39;49;00m])\r\n",
      "\r\n",
      "    train_df.write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(path=s3_output_train_data)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWrote to output file:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_train_data))\r\n",
      "    \r\n",
      "    validation_df.write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(path=s3_output_validation_data)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWrote to output file:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_validation_data))\r\n",
      "\r\n",
      "    test_df.write.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).save(path=s3_output_test_data)    \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWrote to output file:  \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(s3_output_test_data))\r\n",
      "\r\n",
      "    restored_test_df = spark.read.format(\u001b[33m'\u001b[39;49;00m\u001b[33mtfrecords\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).option(\u001b[33m'\u001b[39;49;00m\u001b[33mrecordType\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mExample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).load(path=s3_output_test_data)\r\n",
      "    restored_test_df.show()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\r\n",
      "    spark = SparkSession.builder.appName(\u001b[33m'\u001b[39;49;00m\u001b[33mAmazonReviewsSparkProcessor\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).getOrCreate()\r\n",
      "\r\n",
      "    \u001b[37m# Convert command line args into a map of args\u001b[39;49;00m\r\n",
      "    args_iter = \u001b[36miter\u001b[39;49;00m(sys.argv[\u001b[34m1\u001b[39;49;00m:])\r\n",
      "    args = \u001b[36mdict\u001b[39;49;00m(\u001b[36mzip\u001b[39;49;00m(args_iter, args_iter))\r\n",
      "\r\n",
      "    \u001b[37m# Retrieve the args and replace 's3://' with 's3a://' (used by Spark)\u001b[39;49;00m\r\n",
      "    s3_input_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_input_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_input_data)\r\n",
      "\r\n",
      "    s3_output_train_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_train_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_train_data)\r\n",
      "\r\n",
      "    s3_output_validation_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_validation_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_validation_data)\r\n",
      "\r\n",
      "    s3_output_test_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_test_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_test_data)\r\n",
      "\r\n",
      "    transform(spark, \r\n",
      "              s3_input_data, \r\n",
      "              \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output/bert/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "              \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output/bert/validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "              \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing/output/bert/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "        \u001b[37m# s3_output_train_data, s3_output_validation_data, s3_output_test_data\u001b[39;49;00m\r\n",
      "    )\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize preprocess-spark-text-to-bert.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "processor = ScriptProcessor(base_job_name='spark-amazon-reviews-processor',\n",
    "                            image_uri=image_uri,\n",
    "                            command=['/opt/program/submit'],\n",
    "                            role=role,\n",
    "                            instance_count=2, # instance_count needs to be > 1 or you will see the following error:  \"INFO yarn.Client: Application report for application_ (state: ACCEPTED)\"\n",
    "                            instance_type='ml.r5.xlarge',\n",
    "                            env={'mode': 'python'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "output_prefix = 'amazon-reviews-spark-processor-{}'.format(timestamp_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-140773038493/amazon-reviews-spark-processor-2020-07-25-18-42-56/output/bert-train\n",
      "s3://sagemaker-us-west-2-140773038493/amazon-reviews-spark-processor-2020-07-25-18-42-56/output/bert-validation\n",
      "s3://sagemaker-us-west-2-140773038493/amazon-reviews-spark-processor-2020-07-25-18-42-56/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "train_data_bert_output = 's3://{}/{}/output/bert-train'.format(bucket, output_prefix)\n",
    "validation_data_bert_output = 's3://{}/{}/output/bert-validation'.format(bucket, output_prefix)\n",
    "test_data_bert_output = 's3://{}/{}/output/bert-test'.format(bucket, output_prefix)\n",
    "\n",
    "print(train_data_bert_output)\n",
    "print(validation_data_bert_output)\n",
    "print(test_data_bert_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-amazon-reviews-processor-2020-07-25-18-42-56-429\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/spark-amazon-reviews-processor-2020-07-25-18-42-56-429/input/code/preprocess-spark-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/spark-amazon-reviews-processor-2020-07-25-18-42-56-429/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/spark-amazon-reviews-processor-2020-07-25-18-42-56-429/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/spark-amazon-reviews-processor-2020-07-25-18-42-56-429/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "processor.run(code='preprocess-spark-text-to-bert.py',\n",
    "              arguments=['s3_input_data', s3_input_data,\n",
    "                         's3_output_train_data', train_data_bert_output,\n",
    "                         's3_output_validation_data', validation_data_bert_output,\n",
    "                         's3_output_test_data', test_data_bert_output,                         \n",
    "              ],\n",
    "              # We need this dummy output to allow us to call \n",
    "              #    ProcessingJob.from_processing_name() later \n",
    "              #    to describe the job and poll for Completed status                            \n",
    "              outputs=[\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-train',\n",
    "                                        source='/opt/ml/processing/output/bert/train'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-validation',\n",
    "                                        source='/opt/ml/processing/output/bert/validation'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-test',\n",
    "                                        source='/opt/ml/processing/output/bert/test'),\n",
    "              ],          \n",
    "              logs=True,\n",
    "              wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-west-2#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=spark-amazon-reviews-processor-2020-07-25-18-42-56-429;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "spark_processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, spark_processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-west-2-140773038493/amazon-reviews-spark-processor-2020-07-25-18-42-56/?region=us-west-2&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# This is different than the job name because we are not using ProcessingOutput's in this Spark ML case.\n",
    "spark_processing_job_s3_output_prefix = output_prefix\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>'.format(bucket, spark_processing_job_s3_output_prefix, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Processing Jobs through boto3 Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingJobSummaries': [{'ProcessingJobName': 'spark-amazon-reviews-processor-2020-07-25-18-42-56-429',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:140773038493:processing-job/spark-amazon-reviews-processor-2020-07-25-18-42-56-429',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 25, 18, 42, 56, 878000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 25, 18, 42, 56, 878000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'InProgress'},\n",
       "  {'ProcessingJobName': 'sagemaker-scikit-learn-2020-07-25-18-35-13-587',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:140773038493:processing-job/sagemaker-scikit-learn-2020-07-25-18-35-13-587',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 25, 18, 35, 14, 34000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 7, 25, 18, 39, 29, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 25, 18, 39, 29, 281000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'spark-amazon-reviews-analyzer-2020-07-25-18-24-54-247',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:140773038493:processing-job/spark-amazon-reviews-analyzer-2020-07-25-18-24-54-247',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 25, 18, 24, 54, 670000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 7, 25, 18, 31, 3, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 25, 18, 31, 3, 845000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'pr-1-c4f44ecca3fb4f218b58302e584c68f02d460f75f7f04dd68d369ab9ab',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:140773038493:processing-job/pr-1-c4f44ecca3fb4f218b58302e584c68f02d460f75f7f04dd68d369ab9ab',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 25, 16, 20, 31, 260000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 7, 25, 16, 25, 11, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 25, 16, 25, 11, 834000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'db-1-f0c237a12a5c4634bbd1f16b082fd142610fe48573454ed98d3d2268d7',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:140773038493:processing-job/db-1-f0c237a12a5c4634bbd1f16b082fd142610fe48573454ed98d3d2268d7',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 25, 16, 16, 44, 9000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 7, 25, 16, 20, 12, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 25, 16, 20, 12, 65000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'}],\n",
       " 'ResponseMetadata': {'RequestId': '8e33b9e5-5ac8-4276-85a6-bb4e3f2d0690',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '8e33b9e5-5ac8-4276-85a6-bb4e3f2d0690',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1746',\n",
       "   'date': 'Sat, 25 Jul 2020 18:42:56 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "client.list_processing_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Wait Until the Processing Job Completes\n",
    "Re-run this next cell until the job status shows `Completed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/spark-amazon-reviews-processor-2020-07-25-18-42-56-429/input/code/preprocess-spark-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/spark-amazon-reviews-processor-2020-07-25-18-42-56-429/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/spark-amazon-reviews-processor-2020-07-25-18-42-56-429/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/spark-amazon-reviews-processor-2020-07-25-18-42-56-429/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'spark-amazon-reviews-processor-2020-07-25-18-42-56-429', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.r5.xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '140773038493.dkr.ecr.us-west-2.amazonaws.com/amazon-reviews-spark-processor:latest', 'ContainerEntrypoint': ['/opt/program/submit', '/opt/ml/processing/input/code/preprocess-spark-text-to-bert.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-west-2-140773038493/amazon-reviews-pds/tsv/', 's3_output_train_data', 's3://sagemaker-us-west-2-140773038493/amazon-reviews-spark-processor-2020-07-25-18-42-56/output/bert-train', 's3_output_validation_data', 's3://sagemaker-us-west-2-140773038493/amazon-reviews-spark-processor-2020-07-25-18-42-56/output/bert-validation', 's3_output_test_data', 's3://sagemaker-us-west-2-140773038493/amazon-reviews-spark-processor-2020-07-25-18-42-56/output/bert-test']}, 'Environment': {'mode': 'python'}, 'RoleArn': 'arn:aws:iam::140773038493:role/TeamRole', 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:140773038493:processing-job/spark-amazon-reviews-processor-2020-07-25-18-42-56-429', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 7, 25, 18, 42, 56, 878000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 7, 25, 18, 42, 56, 878000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '26b10ff0-b72d-4748-908d-fc8edb482cde', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '26b10ff0-b72d-4748-908d-fc8edb482cde', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2420', 'date': 'Sat, 25 Jul 2020 18:42:56 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=spark_processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Please Wait Until the ^^ Processing Job ^^ Completes Above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................\u001b[35mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: /usr/hadoop-3.2.1/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: Use of this script to start YARN daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"yarn --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.2.1/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:25,459 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.154.221\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.2.1/etc/hadoop:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-aws-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.375.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-tests-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/had\u001b[0m\n",
      "\u001b[34moop-yarn-server-router-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_262\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:25,469 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:25,548 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-a8e235d6-c613-48c9-b4ce-88aa88245d6a\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:25,954 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:25,967 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:25,968 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:25,968 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:25,972 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:25,973 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:25,973 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:25,973 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,015 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,025 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,025 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,029 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,032 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Jul 25 18:46:26\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,033 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,033 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,034 INFO util.GSet: 2.0% max memory 6.7 GB = 136.4 MB\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,034 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,075 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,075 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,082 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,082 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,082 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,082 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,082 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,083 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,083 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,083 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,083 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,083 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,083 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,107 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,107 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,107 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,107 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,120 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,120 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,120 INFO util.GSet: 1.0% max memory 6.7 GB = 68.2 MB\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,120 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,138 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,138 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,138 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,138 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,143 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,144 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,148 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,148 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,148 INFO util.GSet: 0.25% max memory 6.7 GB = 17.0 MB\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,148 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,156 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,156 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,156 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,159 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,159 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,160 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,160 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,161 INFO util.GSet: 0.029999999329447746% max memory 6.7 GB = 2.0 MB\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,161 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,181 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1992295823-10.0.154.221-1595702786174\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,194 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,217 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,301 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,312 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,316 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:26,316 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.154.221\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34mStarting namenodes on [algo-1]\u001b[0m\n",
      "\u001b[34malgo-1: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting datanodes\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting secondary namenodes [ip-10-0-154-221.us-west-2.compute.internal]\u001b[0m\n",
      "\u001b[34mip-10-0-154-221.us-west-2.compute.internal: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mStarting resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mStarting nodemanagers\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:38,901 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:39.856544: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:39.856643: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:39.856653: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\u001b[0m\n",
      "\u001b[34m2.1.0\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading:  15%|        | 34.8k/232k [00:00<00:00, 270kB/s]#015Downloading:  90%| | 209k/232k [00:00<00:00, 356kB/s] #015Downloading: 100%|| 232k/232k [00:00<00:00, 890kB/s]\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:41,877 INFO spark.SparkContext: Running Spark version 2.4.6\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:41,896 INFO spark.SparkContext: Submitted application: AmazonReviewsSparkProcessor\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:41,934 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:41,934 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:41,934 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:41,935 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:41,935 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,127 INFO util.Utils: Successfully started service 'sparkDriver' on port 41487.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,152 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,166 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,168 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,168 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,175 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-0046fc1d-72ce-491f-9b6c-edfde291f085\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,188 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,228 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,294 INFO util.log: Logging initialized @4449ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,347 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,361 INFO server.Server: Started @4517ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,375 INFO server.AbstractConnector: Started ServerConnector@64c5b64d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,375 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,398 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ca34911{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,399 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4c29e009{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,399 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9105102{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,402 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@101e2dfe{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,403 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b0df6be{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,404 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21f49969{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,404 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@17873e88{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,406 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@761ced2{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,406 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@29bcd3c8{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,407 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@63639bfc{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,407 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@38e26589{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,408 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@317c3b19{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,409 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e918c6d{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,409 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d25438{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,410 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ab7b7a6{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,411 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7ac3f51a{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,412 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@843eccb{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,412 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ba30a6f{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,413 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@cbe943a{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,414 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69b0cb92{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,421 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7e8247db{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,422 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@19e39551{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,423 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@58945a2a{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,424 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46cc8d19{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,425 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ade8c9f{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:42,427 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.154.221:4040\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:43,119 INFO client.RMProxy: Connecting to ResourceManager at /10.0.154.221:8032\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:43,464 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:43,526 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:43,526 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:43,540 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (31706 MB per container)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:43,541 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:43,541 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:43,545 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:43,551 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:43,598 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:44,756 INFO yarn.Client: Uploading resource file:/tmp/spark-1dc54830-46da-4131-8d66-30cfd6f3780d/__spark_libs__5163351121890235188.zip -> hdfs://10.0.154.221/user/root/.sparkStaging/application_1595702797334_0001/__spark_libs__5163351121890235188.zip\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:44,905 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:45,670 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:45,896 INFO yarn.Client: Uploading resource file:/usr/spark-2.4.6/python/lib/pyspark.zip -> hdfs://10.0.154.221/user/root/.sparkStaging/application_1595702797334_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:45,904 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:45,926 INFO yarn.Client: Uploading resource file:/usr/spark-2.4.6/python/lib/py4j-0.10.7-src.zip -> hdfs://10.0.154.221/user/root/.sparkStaging/application_1595702797334_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:45,933 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:46,070 INFO yarn.Client: Uploading resource file:/tmp/spark-1dc54830-46da-4131-8d66-30cfd6f3780d/__spark_conf__2651253930766588901.zip -> hdfs://10.0.154.221/user/root/.sparkStaging/application_1595702797334_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:46,082 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:46,118 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:46,118 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:46,118 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:46,118 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:46,118 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:46,967 INFO yarn.Client: Submitting application application_1595702797334_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:47,174 INFO impl.YarnClientImpl: Submitted application application_1595702797334_0001\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:47,177 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1595702797334_0001 and attemptId None\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:48,186 INFO yarn.Client: Application report for application_1595702797334_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:48,189 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1595702807064\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1595702797334_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:49,191 INFO yarn.Client: Application report for application_1595702797334_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:50,194 INFO yarn.Client: Application report for application_1595702797334_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:51,196 INFO yarn.Client: Application report for application_1595702797334_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:52,199 INFO yarn.Client: Application report for application_1595702797334_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:52,709 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1595702797334_0001), /proxy/application_1595702797334_0001\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:52,893 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:53,201 INFO yarn.Client: Application report for application_1595702797334_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:53,202 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.190.110\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1595702807064\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1595702797334_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:53,203 INFO cluster.YarnClientSchedulerBackend: Application application_1595702797334_0001 has started running.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:53,232 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44581.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:53,233 INFO netty.NettyBlockTransferService: Server created on 10.0.154.221:44581\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:53,234 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:53,250 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.154.221, 44581, None)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:53,252 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.154.221:44581 with 366.3 MB RAM, BlockManagerId(driver, 10.0.154.221, 44581, None)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:53,254 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.154.221, 44581, None)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:53,255 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.154.221, 44581, None)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-07-25 18:46:53,352 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:53,359 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f22c55e{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:55,838 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.190.110:58510) with ID 1\u001b[0m\n",
      "\u001b[34m2020-07-25 18:46:55,925 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-2:35743 with 11.9 GB RAM, BlockManagerId(1, algo-2, 35743, None)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:12,506 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:12,734 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/spark-2.4.6/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:12,734 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-2.4.6/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:12,740 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:12,740 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@325f777a{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:12,741 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:12,741 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a7aa55f{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:12,742 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:12,742 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1ffe424{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:12,743 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:12,743 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d85a42b{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:12,744 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:12,745 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53dc6e75{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:13,076 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-west-2-140773038493/amazon-reviews-pds/tsv/\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-west-2-140773038493/amazon-reviews-spark-processor-2020-07-25-18-42-56/output/bert-train\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-west-2-140773038493/amazon-reviews-spark-processor-2020-07-25-18-42-56/output/bert-validation\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-west-2-140773038493/amazon-reviews-spark-processor-2020-07-25-18-42-56/output/bert-test\u001b[0m\n",
      "\u001b[34mProcessing s3a://sagemaker-us-west-2-140773038493/amazon-reviews-pds/tsv/ => /opt/ml/processing/output/bert/train\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:13,256 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:13,303 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:13,303 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:14,601 INFO datasources.InMemoryFileIndex: It took 102 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,025 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,031 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,033 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,039 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,422 INFO codegen.CodeGenerator: Code generated in 228.754414 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,624 INFO codegen.CodeGenerator: Code generated in 37.304555 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,673 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 401.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,720 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 42.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,721 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.154.221:44581 (size: 42.9 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,724 INFO spark.SparkContext: Created broadcast 0 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,745 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,827 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,844 INFO scheduler.DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,844 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,845 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,846 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:15,850 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:16,019 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 16.3 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:16,021 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.3 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:16,022 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.154.221:44581 (size: 7.3 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:16,023 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:16,035 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:16,036 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:16,063 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:16,271 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:35743 (size: 7.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:16,823 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:35743 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:18,941 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2886 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:18,944 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:18,948 INFO scheduler.DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 3.084 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:18,952 INFO scheduler.DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 3.124397 s\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|   product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|         US|   21269168| RSH1OZ87OYK92|B013PURRZW|     603406193|Madden NFL 16 - X...|Digital_Video_Games|          2|            2|          3|   N|                N|A slight improvem...|I keep buying mad...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|     133437|R1WFOQ3N9BO65I|B00F4CEHNK|     341969535| Xbox Live Gift Card|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|             Awesome| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   45765011| R3YOOS71KM5M9|B00DNHLFQA|     951665344|Command & Conquer...|Digital_Video_Games|          5|            0|          0|   N|                Y|Hail to the great...|If you are preppi...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|     113118|R3R14UATT3OUFU|B004RMK5QG|     395682204|Playstation Plus ...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|             Perfect| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364| RV2W9SGDNQA2C|B00G9BNLQE|     640460561|Saints Row IV - E...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364|R3CFKLIZ0I2KOB|B00IMIL498|     621922192|Double Dragon: Ne...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   38426028|R1LRYU1V0T3O38|B00S00IJH8|     215163395|              Sims 4|Digital_Video_Games|          4|            0|          0|   N|                Y|i like the new sk...|i like the new sk...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    6057518| R44QKV6FE5CJ2|B004RMK4BC|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|               Super| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   20715661|R2TX1KLPXXXNYS|B00K59HKIQ|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|         Easy & Fast|Excellent, fast a...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   26540306|R1JEEW4C6R89BA|B00K59HKIQ|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|                  Ok| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    8926809|R3B3UHK1FO0ERS|B004774IPU|     151985175|Sid Meier's Civil...|Digital_Video_Games|          1|            0|          0|   N|                N|I am still playin...|As has been writt...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   31525534|R2GVSDHW513SS1|B002LIT9EC|     695277014|Build-a-lot 4: Po...|Digital_Video_Games|          5|            0|          0|   N|                Y|Probably the best...|Probably the best...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364|R1R1NT516PYT73|B008ALUBYQ|     112160022|       Borderlands 2|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22977584|R3K624QDQKENN9|B010KYDNDG|     835376637|Minecraft for PC/...|Digital_Video_Games|          4|            0|          0|   N|                Y|                 FUN|COOL BUT IT LAGES...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   45765011| R1FOXH7PCJX3V|B008ALUBYQ|     112160022|       Borderlands 2|Digital_Video_Games|          1|            0|          2|   N|                Y|            One Star|Lames purchase I ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    2239522| RA1246M1OMDWC|B004RMK4P8|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|               Great| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   48805811|R2I9SXWB0PAEKQ|B004RMK5QG|     395682204|Playstation Plus ...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|          Awesome!!!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   18646481|R3UGL544NA0G9C|B00BI16Z22|     552981447|Brink of Consciou...|Digital_Video_Games|          4|            0|          0|   N|                Y|       worth playing|pretty good but n...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   10310935|R1CBA4Y92GVAVM|B004VSTQ2A|     232803743|Xbox Live Subscri...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|What can I say......| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    5587610|R24NEKNR01VEHU|B00GAC1D2G|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|        Just amazing|Very fast to rece...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34mShowing null review_body rows...\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,032 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,033 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnull(review_body#13)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,034 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,035 INFO execution.FileSourceScanExec: Pushed Filters: IsNull(review_body)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,103 INFO codegen.CodeGenerator: Code generated in 29.196353 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,111 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 401.9 KB, free 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,129 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 42.9 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,130 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.154.221:44581 (size: 42.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,131 INFO spark.SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,132 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,143 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,144 INFO scheduler.DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,144 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,144 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,145 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,145 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,150 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 16.6 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,152 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.4 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,152 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.154.221:44581 (size: 7.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,153 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,154 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,154 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,155 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,190 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:35743 (size: 7.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:19,244 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:35743 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,459 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2304 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,459 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,460 INFO scheduler.DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 2.314 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,461 INFO scheduler.DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 2.317670 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,463 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,464 INFO scheduler.DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,464 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,464 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,464 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,465 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,467 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 16.6 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,468 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.4 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,469 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.154.221:44581 (size: 7.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,470 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,470 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[7] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,470 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,471 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:21,483 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-2:35743 (size: 7.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:22,867 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1396 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:22,868 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:22,868 INFO scheduler.DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 1.403 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:22,869 INFO scheduler.DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 1.405470 s\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+-----------+-----------+\u001b[0m\n",
      "\u001b[34m|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|   product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|review_body|review_date|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+-----------+-----------+\u001b[0m\n",
      "\u001b[34m|         US|   44947401| RKHI9WDB7S5WR|B008D7F47Q|     886655228|      FIFA Soccer 13|Digital_Video_Games|          5|            0|          0|   N|                N|          Five Stars|       null| 2015-05-08|\u001b[0m\n",
      "\u001b[34m|         US|    4190665|R2UJ6J6TUON90A|B00S00IJH8|     215163395|              Sims 4|Digital_Video_Games|          2|            0|          2|   N|                N|           Two Stars|       null| 2015-04-14|\u001b[0m\n",
      "\u001b[34m|         US|   51305484| RJVXCTIK9SIC0|B0040V4T8O|      97283480|Medieval II: Tota...|Digital_Video_Games|          5|            0|          0|   N|                Y|It's an amazing g...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   48052209| R8PISK7HHT6BH|B004ZUFKEC|     321025745|The Sims 3 [Mac D...|Digital_Video_Games|          1|            0|          0|   N|                N| What a good wast...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   17134921|R2H22164810P1S|B007Z3RN2I|     137803476|Call of Duty: Bla...|Digital_Video_Games|          3|            0|          1|   N|                Y|I won't be wronge...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   47668954|R1N1YQ8RZOGAMP|B00506X3Y4|     692311664|  Duke Nukem Forever|Digital_Video_Games|          5|            1|          2|   N|                Y|What you waiting ...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   48104724|R2WUQWZTYWZYWY|B00PG8FFFQ|     245597084|Block Financial H...|   Digital_Software|          1|            0|          0|   N|                Y| I am very disapp...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   49035882|R3A8HHF16GNB9M|B005WX2X1Y|      35439242|        Rostta Stone|   Digital_Software|          1|            1|          4|   N|                Y| Very disappointe...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   46107169|R1G1KQDXWWAZGU|B00NG7JVSQ|     811978073|TurboTax Deluxe F...|   Digital_Software|          5|            0|          2|   N|                N| just as I would ...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   18699318| RVYUT2W57G45K|B00NG7K2RA|     349370473|TurboTax Premier ...|   Digital_Software|          1|           24|         39|   N|                Y| Huge waste of ti...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   14757352|R1EAMCG69CDL91|B00I9FX20S|     676852534|Dupe Eliminator 1...|   Digital_Software|          5|            0|          0|   N|                N|The best tool for...|       null|       null|\u001b[0m\n",
      "\u001b[34m|         US|   52995039|R1VR3Q470S6Y5Q|B008S0IMCC|     534964191| Quicken Deluxe 2013|   Digital_Software|          1|            0|          0|   N|                Y|Tis Pity#011I have b...|       null|       null|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+-----------+-----------+\n",
      "\u001b[0m\n",
      "\u001b[34mShowing cleaned csv\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:22,919 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:22,919 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:22,920 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:22,920 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:22,984 INFO codegen.CodeGenerator: Code generated in 28.11377 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:22,989 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 401.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,002 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 42.9 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,003 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.154.221:44581 (size: 42.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,003 INFO spark.SparkContext: Created broadcast 5 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,004 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,012 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,013 INFO scheduler.DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,013 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,014 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,014 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,014 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,018 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 16.8 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,019 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.5 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,020 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.154.221:44581 (size: 7.5 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,020 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,021 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,021 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,022 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,033 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:35743 (size: 7.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,083 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:35743 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,142 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 120 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,142 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,143 INFO scheduler.DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.128 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,144 INFO scheduler.DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.131339 s\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|   product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34m|         US|   21269168| RSH1OZ87OYK92|B013PURRZW|     603406193|Madden NFL 16 - X...|Digital_Video_Games|          2|            2|          3|   N|                N|A slight improvem...|I keep buying mad...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|     133437|R1WFOQ3N9BO65I|B00F4CEHNK|     341969535| Xbox Live Gift Card|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|             Awesome| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   45765011| R3YOOS71KM5M9|B00DNHLFQA|     951665344|Command & Conquer...|Digital_Video_Games|          5|            0|          0|   N|                Y|Hail to the great...|If you are preppi...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|     113118|R3R14UATT3OUFU|B004RMK5QG|     395682204|Playstation Plus ...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|             Perfect| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364| RV2W9SGDNQA2C|B00G9BNLQE|     640460561|Saints Row IV - E...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364|R3CFKLIZ0I2KOB|B00IMIL498|     621922192|Double Dragon: Ne...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   38426028|R1LRYU1V0T3O38|B00S00IJH8|     215163395|              Sims 4|Digital_Video_Games|          4|            0|          0|   N|                Y|i like the new sk...|i like the new sk...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    6057518| R44QKV6FE5CJ2|B004RMK4BC|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|               Super| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   20715661|R2TX1KLPXXXNYS|B00K59HKIQ|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|         Easy & Fast|Excellent, fast a...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   26540306|R1JEEW4C6R89BA|B00K59HKIQ|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|                  Ok| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    8926809|R3B3UHK1FO0ERS|B004774IPU|     151985175|Sid Meier's Civil...|Digital_Video_Games|          1|            0|          0|   N|                N|I am still playin...|As has been writt...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   31525534|R2GVSDHW513SS1|B002LIT9EC|     695277014|Build-a-lot 4: Po...|Digital_Video_Games|          5|            0|          0|   N|                Y|Probably the best...|Probably the best...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22151364|R1R1NT516PYT73|B008ALUBYQ|     112160022|       Borderlands 2|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|            Awesome!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   22977584|R3K624QDQKENN9|B010KYDNDG|     835376637|Minecraft for PC/...|Digital_Video_Games|          4|            0|          0|   N|                Y|                 FUN|COOL BUT IT LAGES...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   45765011| R1FOXH7PCJX3V|B008ALUBYQ|     112160022|       Borderlands 2|Digital_Video_Games|          1|            0|          2|   N|                Y|            One Star|Lames purchase I ...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    2239522| RA1246M1OMDWC|B004RMK4P8|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|               Great| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   48805811|R2I9SXWB0PAEKQ|B004RMK5QG|     395682204|Playstation Plus ...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|          Awesome!!!| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   18646481|R3UGL544NA0G9C|B00BI16Z22|     552981447|Brink of Consciou...|Digital_Video_Games|          4|            0|          0|   N|                Y|       worth playing|pretty good but n...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|   10310935|R1CBA4Y92GVAVM|B004VSTQ2A|     232803743|Xbox Live Subscri...|Digital_Video_Games|          5|            0|          0|   N|                Y|          Five Stars|What can I say......| 2015-08-31|\u001b[0m\n",
      "\u001b[34m|         US|    5587610|R24NEKNR01VEHU|B00GAC1D2G|     384246568|Playstation Netwo...|Digital_Video_Games|          5|            0|          0|   N|                Y|        Just amazing|Very fast to rece...| 2015-08-31|\u001b[0m\n",
      "\u001b[34m+-----------+-----------+--------------+----------+--------------+--------------------+-------------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,188 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,188 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,189 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,189 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,210 INFO codegen.CodeGenerator: Code generated in 10.167871 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,224 INFO codegen.CodeGenerator: Code generated in 10.405883 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,228 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 401.9 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,247 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 42.9 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,247 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.154.221:44581 (size: 42.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,248 INFO spark.SparkContext: Created broadcast 7 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,249 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,257 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,258 INFO scheduler.DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,258 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,258 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,259 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,259 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,262 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 12.2 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,263 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.4 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,263 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.154.221:44581 (size: 6.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,264 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,265 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,265 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,266 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,278 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-2:35743 (size: 6.4 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-07-25 18:47:23,318 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:35743 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,424 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 158 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,425 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,425 INFO scheduler.DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 0.165 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,426 INFO scheduler.DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 0.168150 s\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+\u001b[0m\n",
      "\u001b[34m|star_rating|         review_body|\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+\u001b[0m\n",
      "\u001b[34m|          2|I keep buying mad...|\u001b[0m\n",
      "\u001b[34m|          5|             Awesome|\u001b[0m\n",
      "\u001b[34m|          5|If you are preppi...|\u001b[0m\n",
      "\u001b[34m|          5|             Perfect|\u001b[0m\n",
      "\u001b[34m|          5|            Awesome!|\u001b[0m\n",
      "\u001b[34m|          5|            Awesome!|\u001b[0m\n",
      "\u001b[34m|          4|i like the new sk...|\u001b[0m\n",
      "\u001b[34m|          5|               Super|\u001b[0m\n",
      "\u001b[34m|          5|Excellent, fast a...|\u001b[0m\n",
      "\u001b[34m|          5|                  Ok|\u001b[0m\n",
      "\u001b[34m|          1|As has been writt...|\u001b[0m\n",
      "\u001b[34m|          5|Probably the best...|\u001b[0m\n",
      "\u001b[34m|          5|            Awesome!|\u001b[0m\n",
      "\u001b[34m|          4|COOL BUT IT LAGES...|\u001b[0m\n",
      "\u001b[34m|          1|Lames purchase I ...|\u001b[0m\n",
      "\u001b[34m|          5|               Great|\u001b[0m\n",
      "\u001b[34m|          5|          Awesome!!!|\u001b[0m\n",
      "\u001b[34m|          4|pretty good but n...|\u001b[0m\n",
      "\u001b[34m|          5|What can I say......|\u001b[0m\n",
      "\u001b[34m|          5|Very fast to rece...|\u001b[0m\n",
      "\u001b[34m+-----------+--------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,763 INFO spark.ContextCleaner: Cleaned accumulator 2\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,763 INFO spark.ContextCleaner: Cleaned accumulator 52\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,763 INFO spark.ContextCleaner: Cleaned accumulator 16\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,777 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.154.221:44581 in memory (size: 7.5 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,779 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:35743 in memory (size: 7.5 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,787 INFO spark.ContextCleaner: Cleaned accumulator 128\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,787 INFO spark.ContextCleaner: Cleaned accumulator 141\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,788 INFO spark.ContextCleaner: Cleaned accumulator 39\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,788 INFO spark.ContextCleaner: Cleaned accumulator 42\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,788 INFO spark.ContextCleaner: Cleaned accumulator 20\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,788 INFO spark.ContextCleaner: Cleaned accumulator 143\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,788 INFO spark.ContextCleaner: Cleaned accumulator 28\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,788 INFO spark.ContextCleaner: Cleaned accumulator 107\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,788 INFO spark.ContextCleaner: Cleaned accumulator 135\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,788 INFO spark.ContextCleaner: Cleaned accumulator 81\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,792 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.154.221:44581 in memory (size: 42.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,793 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-2:35743 in memory (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,802 INFO spark.ContextCleaner: Cleaned accumulator 97\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,803 INFO spark.ContextCleaner: Cleaned accumulator 40\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,803 INFO spark.ContextCleaner: Cleaned accumulator 103\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,803 INFO spark.ContextCleaner: Cleaned accumulator 125\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,803 INFO spark.ContextCleaner: Cleaned accumulator 124\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,803 INFO spark.ContextCleaner: Cleaned accumulator 91\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,803 INFO spark.ContextCleaner: Cleaned accumulator 62\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,803 INFO spark.ContextCleaner: Cleaned accumulator 139\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,803 INFO spark.ContextCleaner: Cleaned accumulator 131\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,803 INFO spark.ContextCleaner: Cleaned accumulator 4\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,803 INFO spark.ContextCleaner: Cleaned accumulator 78\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,803 INFO spark.ContextCleaner: Cleaned accumulator 18\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,806 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-2:35743 in memory (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,808 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.154.221:44581 in memory (size: 42.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 12\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 33\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 113\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 66\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 57\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 59\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 108\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 49\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 92\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 69\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 88\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 76\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 118\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 119\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 142\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 38\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 115\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 84\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 19\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 87\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,819 INFO spark.ContextCleaner: Cleaned accumulator 22\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,820 INFO spark.ContextCleaner: Cleaned accumulator 7\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,820 INFO spark.ContextCleaner: Cleaned accumulator 63\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,820 INFO spark.ContextCleaner: Cleaned accumulator 6\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,820 INFO spark.ContextCleaner: Cleaned accumulator 85\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,820 INFO spark.ContextCleaner: Cleaned accumulator 50\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,820 INFO spark.ContextCleaner: Cleaned accumulator 8\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,820 INFO spark.ContextCleaner: Cleaned accumulator 89\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,820 INFO spark.ContextCleaner: Cleaned accumulator 100\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,820 INFO spark.ContextCleaner: Cleaned accumulator 144\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,820 INFO spark.ContextCleaner: Cleaned accumulator 21\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,820 INFO spark.ContextCleaner: Cleaned accumulator 79\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,820 INFO spark.ContextCleaner: Cleaned accumulator 99\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,820 INFO spark.ContextCleaner: Cleaned accumulator 29\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,820 INFO spark.ContextCleaner: Cleaned accumulator 56\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,820 INFO spark.ContextCleaner: Cleaned accumulator 32\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,820 INFO spark.ContextCleaner: Cleaned accumulator 58\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,827 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-2:35743 in memory (size: 7.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,828 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.154.221:44581 in memory (size: 7.3 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,837 INFO spark.ContextCleaner: Cleaned accumulator 111\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,837 INFO spark.ContextCleaner: Cleaned accumulator 41\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,837 INFO spark.ContextCleaner: Cleaned accumulator 3\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,837 INFO spark.ContextCleaner: Cleaned accumulator 104\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,839 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.154.221:44581 in memory (size: 6.4 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,840 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-2:35743 in memory (size: 6.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,851 INFO spark.ContextCleaner: Cleaned accumulator 64\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,851 INFO spark.ContextCleaner: Cleaned accumulator 106\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,851 INFO spark.ContextCleaner: Cleaned accumulator 30\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,851 INFO spark.ContextCleaner: Cleaned accumulator 114\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,851 INFO spark.ContextCleaner: Cleaned accumulator 137\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,851 INFO spark.ContextCleaner: Cleaned accumulator 11\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,851 INFO spark.ContextCleaner: Cleaned accumulator 77\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,851 INFO spark.ContextCleaner: Cleaned accumulator 136\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,851 INFO spark.ContextCleaner: Cleaned accumulator 110\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,851 INFO spark.ContextCleaner: Cleaned accumulator 23\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,851 INFO spark.ContextCleaner: Cleaned accumulator 73\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,851 INFO spark.ContextCleaner: Cleaned accumulator 82\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,851 INFO spark.ContextCleaner: Cleaned accumulator 90\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,851 INFO spark.ContextCleaner: Cleaned accumulator 109\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,851 INFO spark.ContextCleaner: Cleaned accumulator 130\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,851 INFO spark.ContextCleaner: Cleaned accumulator 80\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,854 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.154.221:44581 in memory (size: 42.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,854 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:35743 in memory (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,863 INFO spark.ContextCleaner: Cleaned accumulator 127\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,863 INFO spark.ContextCleaner: Cleaned accumulator 37\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,863 INFO spark.ContextCleaner: Cleaned accumulator 98\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,863 INFO spark.ContextCleaner: Cleaned accumulator 72\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,863 INFO spark.ContextCleaner: Cleaned accumulator 27\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,863 INFO spark.ContextCleaner: Cleaned accumulator 95\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,863 INFO spark.ContextCleaner: Cleaned accumulator 68\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,863 INFO spark.ContextCleaner: Cleaned accumulator 146\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,863 INFO spark.ContextCleaner: Cleaned accumulator 138\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,863 INFO spark.ContextCleaner: Cleaned accumulator 55\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,863 INFO spark.ContextCleaner: Cleaned accumulator 120\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,863 INFO spark.ContextCleaner: Cleaned accumulator 122\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,864 INFO spark.ContextCleaner: Cleaned accumulator 75\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,864 INFO spark.ContextCleaner: Cleaned accumulator 26\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,864 INFO spark.ContextCleaner: Cleaned accumulator 67\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,864 INFO spark.ContextCleaner: Cleaned accumulator 5\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,864 INFO spark.ContextCleaner: Cleaned accumulator 112\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,864 INFO spark.ContextCleaner: Cleaned accumulator 35\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,864 INFO spark.ContextCleaner: Cleaned accumulator 132\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,864 INFO spark.ContextCleaner: Cleaned accumulator 117\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,864 INFO spark.ContextCleaner: Cleaned accumulator 134\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,864 INFO spark.ContextCleaner: Cleaned accumulator 24\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,864 INFO spark.ContextCleaner: Cleaned accumulator 147\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,864 INFO spark.ContextCleaner: Cleaned accumulator 9\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,864 INFO spark.ContextCleaner: Cleaned accumulator 34\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,864 INFO spark.ContextCleaner: Cleaned accumulator 48\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,864 INFO spark.ContextCleaner: Cleaned accumulator 126\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,867 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-2:35743 in memory (size: 7.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,872 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.154.221:44581 in memory (size: 7.4 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,880 INFO spark.ContextCleaner: Cleaned accumulator 25\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,880 INFO spark.ContextCleaner: Cleaned accumulator 60\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,880 INFO spark.ContextCleaner: Cleaned accumulator 94\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,880 INFO spark.ContextCleaner: Cleaned accumulator 129\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,880 INFO spark.ContextCleaner: Cleaned accumulator 13\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,880 INFO spark.ContextCleaner: Cleaned accumulator 145\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,880 INFO spark.ContextCleaner: Cleaned accumulator 101\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,880 INFO spark.ContextCleaner: Cleaned accumulator 17\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,880 INFO spark.ContextCleaner: Cleaned accumulator 15\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,880 INFO spark.ContextCleaner: Cleaned accumulator 46\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,880 INFO spark.ContextCleaner: Cleaned accumulator 71\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,880 INFO spark.ContextCleaner: Cleaned accumulator 83\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,887 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-2:35743 in memory (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,888 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.154.221:44581 in memory (size: 42.9 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,899 INFO spark.ContextCleaner: Cleaned accumulator 47\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,899 INFO spark.ContextCleaner: Cleaned accumulator 51\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,899 INFO spark.ContextCleaner: Cleaned accumulator 121\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,899 INFO spark.ContextCleaner: Cleaned accumulator 93\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,899 INFO spark.ContextCleaner: Cleaned accumulator 10\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,899 INFO spark.ContextCleaner: Cleaned accumulator 148\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,900 INFO spark.ContextCleaner: Cleaned accumulator 45\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,900 INFO spark.ContextCleaner: Cleaned accumulator 14\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,900 INFO spark.ContextCleaner: Cleaned accumulator 53\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,900 INFO spark.ContextCleaner: Cleaned accumulator 36\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,900 INFO spark.ContextCleaner: Cleaned accumulator 70\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,900 INFO spark.ContextCleaner: Cleaned accumulator 86\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,900 INFO spark.ContextCleaner: Cleaned accumulator 54\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,900 INFO spark.ContextCleaner: Cleaned accumulator 43\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,901 INFO spark.ContextCleaner: Cleaned accumulator 140\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,901 INFO spark.ContextCleaner: Cleaned accumulator 1\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,901 INFO spark.ContextCleaner: Cleaned accumulator 74\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,901 INFO spark.ContextCleaner: Cleaned accumulator 116\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,901 INFO spark.ContextCleaner: Cleaned accumulator 133\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,903 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-2:35743 in memory (size: 7.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,914 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.154.221:44581 in memory (size: 7.4 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,921 INFO spark.ContextCleaner: Cleaned accumulator 44\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,921 INFO spark.ContextCleaner: Cleaned accumulator 61\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,921 INFO spark.ContextCleaner: Cleaned accumulator 65\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,921 INFO spark.ContextCleaner: Cleaned accumulator 102\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,921 INFO spark.ContextCleaner: Cleaned accumulator 31\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,921 INFO spark.ContextCleaner: Cleaned accumulator 96\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,921 INFO spark.ContextCleaner: Cleaned accumulator 105\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:23,921 INFO spark.ContextCleaner: Cleaned accumulator 123\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,071 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,071 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,072 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,072 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,090 INFO codegen.CodeGenerator: Code generated in 6.257327 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,119 INFO codegen.CodeGenerator: Code generated in 20.314775 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,132 INFO codegen.CodeGenerator: Code generated in 10.086233 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,138 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 401.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,155 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 42.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,156 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.154.221:44581 (size: 42.9 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,157 INFO spark.SparkContext: Created broadcast 9 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,157 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,216 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,217 INFO scheduler.DAGScheduler: Got job 5 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,217 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,217 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,217 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,218 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[22] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,225 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 849.5 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,229 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 644.8 KB, free 364.4 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,229 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.154.221:44581 (size: 644.8 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,230 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,231 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[22] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,231 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,232 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,244 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-2:35743 (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:24,797 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-2:35743 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:26,965 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 2733 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:26,965 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:26,966 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 49471\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:26,968 INFO scheduler.DAGScheduler: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0) finished in 2.747 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:26,968 INFO scheduler.DAGScheduler: Job 5 finished: showString at NativeMethodAccessorImpl.java:0, took 2.751861 s\u001b[0m\n",
      "\u001b[34m+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m|tfrecords                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\u001b[0m\n",
      "\u001b[34m+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m|[[101, 1045, 2562, 9343, 24890, 2296, 2095, 5327, 2027, 2131, 2067, 2000, 2374, 1012, 2023, 2086, 2544, 2003, 1037, 2210, 2488, 2084, 2197, 2086, 1011, 1011, 2021, 2008, 1005, 1055, 2025, 3038, 2172, 1012, 1996, 2208, 3504, 2307, 1012, 1996, 2069, 2518, 3308, 2007, 1996, 7284, 1010, 2003, 1996, 2126, 1996, 2867, 2024, 2467, 4440, 4691, 2006, 2169, 2060, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 11247, 2003, 2145, 9784, 2091, 2011, 1996, 1038, 4135, 4383, 3653, 1011, 2377, 7711, 1012, 2054, 2109, 2000, 2202, 2048, 11287, 2003, 2085, 1037, 5016, 6770, 2050, 2000, 2131, 2589, 2077, 2019, 7116, 20057, 1996, 3608, 2030, 1996, 2377, 5119, 3216, 2041, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 15386, 6462, 2003, 2067, 1010, 2021, 102], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1]]   |\u001b[0m\n",
      "\u001b[34m|[[101, 12476, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                              |\u001b[0m\n",
      "\u001b[34m|[[101, 2065, 2017, 2024, 17463, 4691, 2005, 1996, 2203, 1997, 1996, 2088, 2023, 2003, 2028, 1997, 2216, 2477, 2008, 2017, 2323, 2031, 5361, 2006, 2115, 1011, 2203, 1011, 1997, 1011, 1996, 1011, 2088, 1011, 6947, 7473, 1012, 16889, 2000, 1996, 2307, 14331, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                  |\u001b[0m\n",
      "\u001b[34m|[[101, 3819, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                               |\u001b[0m\n",
      "\u001b[34m|[[101, 12476, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                            |\u001b[0m\n",
      "\u001b[34m|[[101, 12476, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                            |\u001b[0m\n",
      "\u001b[34m|[[101, 1045, 2066, 1996, 2047, 4813, 2066, 27849, 2964, 1999, 2023, 1010, 1998, 13215, 2003, 4569, 1012, 1045, 2036, 2066, 2035, 1996, 2047, 3857, 5549, 5167, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3]]                                                                                                                                                                                                                                                                                                                     |\u001b[0m\n",
      "\u001b[34m|[[101, 3565, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                               |\u001b[0m\n",
      "\u001b[34m|[[101, 6581, 1010, 3435, 1998, 5851, 999, 999, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                             |\u001b[0m\n",
      "\u001b[34m|[[101, 7929, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                               |\u001b[0m\n",
      "\u001b[34m|[[101, 2004, 2038, 2042, 2517, 2011, 2061, 2116, 2500, 1010, 1045, 2855, 2439, 3037, 1999, 2023, 2208, 1012, 1045, 2572, 2145, 2652, 25022, 2615, 1018, 1998, 2293, 2009, 1012, 2009, 1005, 1055, 1037, 9467, 2138, 1045, 1005, 1049, 3201, 2005, 2019, 4423, 2544, 1997, 25022, 2615, 1018, 1998, 2031, 4741, 2005, 2055, 1037, 5476, 2005, 1037, 2488, 2544, 1997, 2009, 1012, 25022, 2615, 1019, 2001, 2025, 2019, 6622, 2021, 1037, 2561, 2128, 26373, 1998, 2009, 2439, 2035, 2008, 2001, 2204, 1999, 25022, 2615, 1018, 1012, 1045, 2428, 3246, 2008, 2043, 25022, 2615, 1020, 3310, 2041, 2027, 2224, 25022, 2615, 1018, 2004, 1996, 3225, 2391, 1998, 5293, 25022, 2615, 1019, 2412, 3047, 1012, 7989, 2008, 2045, 2003, 1037, 2173, 1999, 1996, 3006, 2005, 1037, 5656, 2208, 2008, 7336, 102], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0]]|\u001b[0m\n",
      "\u001b[34m|[[101, 2763, 1996, 2190, 2208, 2005, 4083, 5919, 1997, 2613, 3776, 2800, 1012, 6700, 15794, 2428, 2718, 1996, 3608, 2041, 1997, 1996, 2380, 2007, 2023, 2028, 1010, 2007, 2152, 4547, 3643, 1006, 2004, 2092, 2004, 2019, 14036, 2208, 1007, 1999, 3408, 1997, 2877, 2017, 2083, 1996, 24078, 1997, 2613, 3776, 2458, 1012, 2130, 2295, 2023, 2003, 2195, 2086, 2214, 2113, 1996, 11343, 1997, 22956, 1998, 18726, 2005, 2023, 2208, 2965, 2023, 2003, 2145, 1037, 2442, 2031, 2005, 13007, 4667, 2613, 3776, 9587, 24848, 2015, 1997, 4826, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                        |\u001b[0m\n",
      "\u001b[34m|[[101, 12476, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                            |\u001b[0m\n",
      "\u001b[34m|[[101, 4658, 2021, 2009, 2474, 8449, 2632, 4140, 1997, 1996, 2051, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3]]                                                                                                                                                                                                                                                                                                                                                                    |\u001b[0m\n",
      "\u001b[34m|[[101, 20342, 2015, 5309, 1045, 2471, 2196, 2081, 1012, 1012, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0]]                                                                                                                                                                                                                                                                                                                                                                   |\u001b[0m\n",
      "\u001b[34m|[[101, 2307, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                               |\u001b[0m\n",
      "\u001b[34m|[[101, 12476, 999, 999, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                                        |\u001b[0m\n",
      "\u001b[34m|[[101, 3492, 2204, 2021, 2025, 2004, 2204, 2004, 1996, 2034, 20911, 1997, 8298, 2208, 1011, 1011, 16092, 1026, 7987, 1013, 1028, 3897, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [3]]                                                                                                                                                                                                                                                                                                                                 |\u001b[0m\n",
      "\u001b[34m|[[101, 2054, 2064, 1045, 2360, 1012, 1012, 1012, 12202, 2444, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                                                    |\u001b[0m\n",
      "\u001b[34m|[[101, 2200, 3435, 2000, 4374, 1010, 1998, 1997, 2278, 1037, 3404, 13966, 1998, 3647, 2173, 2000, 4965, 2478, 2115, 4923, 4003, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4]]                                                                                                                                                                                                                                                                                                                                   |\u001b[0m\n",
      "\u001b[34m+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,057 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,058 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,058 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,058 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,083 INFO codegen.CodeGenerator: Code generated in 10.331122 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,108 INFO codegen.CodeGenerator: Code generated in 17.500951 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,116 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 401.9 KB, free 364.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,133 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 42.9 KB, free 364.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,133 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.154.221:44581 (size: 42.9 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,134 INFO spark.SparkContext: Created broadcast 11 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,134 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,157 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,159 INFO scheduler.DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,159 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,159 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,159 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,159 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[29] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,164 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 849.1 KB, free 363.1 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,169 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 644.8 KB, free 362.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,169 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.154.221:44581 (size: 644.8 KB, free: 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,170 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,171 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[29] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,171 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,172 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,183 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-2:35743 (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:27,223 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-2:35743 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,015 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 1844 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,015 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,016 INFO scheduler.DAGScheduler: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0) finished in 1.856 s\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,017 INFO scheduler.DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 1.859579 s\u001b[0m\n",
      "\u001b[34m+--------------------+--------------------+--------------------+---------+\u001b[0m\n",
      "\u001b[34m|           input_ids|          input_mask|         segment_ids|label_ids|\u001b[0m\n",
      "\u001b[34m+--------------------+--------------------+--------------------+---------+\u001b[0m\n",
      "\u001b[34m|[101, 1045, 2562,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [1]|\u001b[0m\n",
      "\u001b[34m|[101, 12476, 102,...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2065, 2017,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 3819, 102, ...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 12476, 999,...|[1, 1, 1, 1, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 12476, 999,...|[1, 1, 1, 1, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 1045, 2066,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [3]|\u001b[0m\n",
      "\u001b[34m|[101, 3565, 102, ...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 6581, 1010,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 7929, 102, ...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2004, 2038,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [0]|\u001b[0m\n",
      "\u001b[34m|[101, 2763, 1996,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 12476, 999,...|[1, 1, 1, 1, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 4658, 2021,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [3]|\u001b[0m\n",
      "\u001b[34m|[101, 20342, 2015...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [0]|\u001b[0m\n",
      "\u001b[34m|[101, 2307, 102, ...|[1, 1, 1, 0, 0, 0...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 12476, 999,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 3492, 2204,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [3]|\u001b[0m\n",
      "\u001b[34m|[101, 2054, 2064,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m|[101, 2200, 3435,...|[1, 1, 1, 1, 1, 1...|[0, 0, 0, 0, 0, 0...|      [4]|\u001b[0m\n",
      "\u001b[34m+--------------------+--------------------+--------------------+---------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,158 INFO spark.ContextCleaner: Cleaned accumulator 208\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,158 INFO spark.ContextCleaner: Cleaned accumulator 197\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,158 INFO spark.ContextCleaner: Cleaned accumulator 160\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,158 INFO spark.ContextCleaner: Cleaned accumulator 159\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,158 INFO spark.ContextCleaner: Cleaned accumulator 166\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,158 INFO spark.ContextCleaner: Cleaned accumulator 190\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,158 INFO spark.ContextCleaner: Cleaned accumulator 193\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,158 INFO spark.ContextCleaner: Cleaned accumulator 198\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,158 INFO spark.ContextCleaner: Cleaned accumulator 172\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,158 INFO spark.ContextCleaner: Cleaned accumulator 210\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,158 INFO spark.ContextCleaner: Cleaned accumulator 183\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,158 INFO spark.ContextCleaner: Cleaned accumulator 182\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,158 INFO spark.ContextCleaner: Cleaned accumulator 170\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,159 INFO spark.ContextCleaner: Cleaned accumulator 174\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,159 INFO spark.ContextCleaner: Cleaned accumulator 207\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,159 INFO spark.ContextCleaner: Cleaned accumulator 203\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,159 INFO spark.ContextCleaner: Cleaned accumulator 187\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,159 INFO spark.ContextCleaner: Cleaned accumulator 180\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,159 INFO spark.ContextCleaner: Cleaned accumulator 176\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,159 INFO spark.ContextCleaner: Cleaned accumulator 202\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,159 INFO spark.ContextCleaner: Cleaned accumulator 212\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,159 INFO spark.ContextCleaner: Cleaned accumulator 150\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,159 INFO spark.ContextCleaner: Cleaned accumulator 164\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,159 INFO spark.ContextCleaner: Cleaned accumulator 169\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,159 INFO spark.ContextCleaner: Cleaned accumulator 171\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,159 INFO spark.ContextCleaner: Cleaned accumulator 200\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,159 INFO spark.ContextCleaner: Cleaned accumulator 165\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,159 INFO spark.ContextCleaner: Cleaned accumulator 177\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,163 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.154.221:44581 in memory (size: 644.8 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,163 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-2:35743 in memory (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,166 INFO spark.ContextCleaner: Cleaned accumulator 189\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 196\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 151\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 149\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 185\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 206\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 209\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 179\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 161\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 211\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 201\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 204\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 181\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 192\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 184\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 194\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 167\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 173\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 195\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 162\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 154\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 199\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 157\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 168\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 191\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 156\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 178\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 175\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 152\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,167 INFO spark.ContextCleaner: Cleaned accumulator 205\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,177 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-2:35743 in memory (size: 644.8 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,187 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.154.221:44581 in memory (size: 644.8 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,232 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,232 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,233 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-2:35743 in memory (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,233 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,233 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,242 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.154.221:44581 in memory (size: 42.9 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,252 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.154.221:44581 in memory (size: 42.9 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,252 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-2:35743 in memory (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,282 INFO codegen.CodeGenerator: Code generated in 34.102377 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,287 INFO spark.ContextCleaner: Cleaned accumulator 188\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,287 INFO spark.ContextCleaner: Cleaned accumulator 158\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,287 INFO spark.ContextCleaner: Cleaned accumulator 155\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,287 INFO spark.ContextCleaner: Cleaned accumulator 186\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,287 INFO spark.ContextCleaner: Cleaned accumulator 153\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,288 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 401.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,300 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 42.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,301 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.154.221:44581 (size: 42.9 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,301 INFO spark.SparkContext: Created broadcast 13 from rdd at DefaultSource.scala:58\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,302 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,380 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,380 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,397 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,398 INFO scheduler.DAGScheduler: Got job 7 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,398 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,398 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,398 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,399 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[38] at map at DefaultSource.scala:58), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,425 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 979.8 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,430 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 692.2 KB, free 364.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,430 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.154.221:44581 (size: 692.2 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,431 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,431 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 7 (MapPartitionsRDD[38] at map at DefaultSource.scala:58) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,431 INFO cluster.YarnScheduler: Adding task set 7.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,432 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,432 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 7.0 (TID 8, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,445 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-2:35743 (size: 692.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 18:47:29,535 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-2:35743 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-07-25 18:57:04,484 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 7.0 (TID 8) in 575051 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:38,947 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 789515 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:38,947 INFO cluster.YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:38,948 INFO scheduler.DAGScheduler: ResultStage 7 (runJob at SparkHadoopWriter.scala:78) finished in 789.548 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:38,948 INFO scheduler.DAGScheduler: Job 7 finished: runJob at SparkHadoopWriter.scala:78, took 789.551438 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:38,968 INFO io.SparkHadoopWriter: Job job_20200725184729_0038 committed.\u001b[0m\n",
      "\u001b[34mWrote to output file:  /opt/ml/processing/output/bert/train\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,012 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,012 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,012 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,013 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,050 INFO codegen.CodeGenerator: Code generated in 28.795524 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,056 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 401.9 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,068 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 42.9 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,068 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.154.221:44581 (size: 42.9 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,069 INFO spark.SparkContext: Created broadcast 15 from rdd at DefaultSource.scala:58\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,070 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,096 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,096 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,111 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,112 INFO scheduler.DAGScheduler: Got job 8 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,112 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,112 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,112 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,112 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[49] at map at DefaultSource.scala:58), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,128 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 979.8 KB, free 362.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,135 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 692.2 KB, free 362.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,135 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.154.221:44581 (size: 692.2 KB, free: 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,135 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,136 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 8 (MapPartitionsRDD[49] at map at DefaultSource.scala:58) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,136 INFO cluster.YarnScheduler: Adding task set 8.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,137 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 9, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,137 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 10, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,149 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-2:35743 (size: 692.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:00:39,186 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-2:35743 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:10:07,040 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 10) in 567903 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,674 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 9) in 783537 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,674 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,675 INFO scheduler.DAGScheduler: ResultStage 8 (runJob at SparkHadoopWriter.scala:78) finished in 783.561 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,675 INFO scheduler.DAGScheduler: Job 8 finished: runJob at SparkHadoopWriter.scala:78, took 783.563920 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,686 INFO io.SparkHadoopWriter: Job job_20200725190039_0049 committed.\u001b[0m\n",
      "\u001b[34mWrote to output file:  /opt/ml/processing/output/bert/validation\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,729 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,730 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, review_body#13)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,730 INFO datasources.FileSourceStrategy: Output Data Schema: struct<star_rating: int, review_body: string>\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,730 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,763 INFO codegen.CodeGenerator: Code generated in 25.105453 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,769 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 401.9 KB, free 361.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,783 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 42.9 KB, free 361.7 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,783 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.154.221:44581 (size: 42.9 KB, free: 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,784 INFO spark.SparkContext: Created broadcast 17 from rdd at DefaultSource.scala:58\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,784 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 18276271 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,815 INFO spark.ContextCleaner: Cleaned accumulator 253\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,815 INFO spark.ContextCleaner: Cleaned accumulator 249\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,815 INFO spark.ContextCleaner: Cleaned accumulator 278\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,815 INFO spark.ContextCleaner: Cleaned accumulator 223\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,816 INFO spark.ContextCleaner: Cleaned accumulator 233\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,816 INFO spark.ContextCleaner: Cleaned accumulator 251\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,816 INFO spark.ContextCleaner: Cleaned accumulator 254\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,816 INFO spark.ContextCleaner: Cleaned accumulator 267\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,816 INFO spark.ContextCleaner: Cleaned accumulator 276\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,816 INFO spark.ContextCleaner: Cleaned accumulator 256\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,817 INFO spark.ContextCleaner: Cleaned accumulator 282\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,820 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-2:35743 in memory (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,822 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,822 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,830 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.154.221:44581 in memory (size: 42.9 KB, free: 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,837 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO scheduler.DAGScheduler: Got job 9 (runJob at SparkHadoopWriter.scala:78) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 266\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 231\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 240\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 236\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 258\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 255\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 269\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 218\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 216\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 270\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 239\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 217\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 234\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 247\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 243\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 265\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 214\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 274\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,838 INFO spark.ContextCleaner: Cleaned accumulator 284\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,839 INFO spark.ContextCleaner: Cleaned accumulator 248\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,839 INFO spark.ContextCleaner: Cleaned accumulator 235\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,839 INFO spark.ContextCleaner: Cleaned accumulator 263\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,839 INFO spark.ContextCleaner: Cleaned accumulator 252\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,839 INFO spark.ContextCleaner: Cleaned accumulator 281\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,839 INFO spark.ContextCleaner: Cleaned accumulator 230\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,839 INFO spark.ContextCleaner: Cleaned accumulator 280\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,839 INFO spark.ContextCleaner: Cleaned accumulator 215\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,839 INFO spark.ContextCleaner: Cleaned accumulator 213\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,839 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,839 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[60] at map at DefaultSource.scala:58), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,845 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.154.221:44581 in memory (size: 692.2 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,847 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-2:35743 in memory (size: 692.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 226\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 219\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 245\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 277\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 262\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 264\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 222\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 271\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 242\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 238\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 237\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 246\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 272\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 260\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 257\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 221\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 244\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 229\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 220\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 273\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 279\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 250\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 224\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 275\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 259\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 225\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 268\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 232\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,851 INFO spark.ContextCleaner: Cleaned accumulator 283\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,858 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-2:35743 in memory (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,859 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.154.221:44581 in memory (size: 42.9 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,863 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.154.221:44581 in memory (size: 692.2 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,863 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-2:35743 in memory (size: 692.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,864 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 979.8 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,871 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 692.3 KB, free 364.2 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,871 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.154.221:44581 (size: 692.3 KB, free: 365.6 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,871 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,872 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[60] at map at DefaultSource.scala:58) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,872 INFO cluster.YarnScheduler: Adding task set 9.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,873 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 11, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8339 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,873 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 12, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8336 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,877 INFO spark.ContextCleaner: Cleaned accumulator 227\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,877 INFO spark.ContextCleaner: Cleaned accumulator 241\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,877 INFO spark.ContextCleaner: Cleaned accumulator 228\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,877 INFO spark.ContextCleaner: Cleaned accumulator 261\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,885 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-2:35743 (size: 692.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:13:42,909 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-2:35743 (size: 42.9 KB, free: 11.9 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-07-25 19:23:11,972 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 12) in 569098 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,625 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 11) in 780753 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,625 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,626 INFO scheduler.DAGScheduler: ResultStage 9 (runJob at SparkHadoopWriter.scala:78) finished in 780.787 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,627 INFO scheduler.DAGScheduler: Job 9 finished: runJob at SparkHadoopWriter.scala:78, took 780.789945 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,642 INFO io.SparkHadoopWriter: Job job_20200725191342_0060 committed.\u001b[0m\n",
      "\u001b[34mWrote to output file:  /opt/ml/processing/output/bert/test\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,661 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 389.7 KB, free 363.9 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,678 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 42.3 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,679 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.154.221:44581 (size: 42.3 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,679 INFO spark.SparkContext: Created broadcast 19 from newAPIHadoopFile at TensorflowRelation.scala:33\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,761 INFO input.FileInputFormat: Total input files to process : 2\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,782 INFO spark.SparkContext: Starting job: aggregate at TensorFlowInferSchema.scala:39\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,783 INFO scheduler.DAGScheduler: Got job 10 (aggregate at TensorFlowInferSchema.scala:39) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,783 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (aggregate at TensorFlowInferSchema.scala:39)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,783 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,783 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,783 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[64] at map at TensorflowRelation.scala:39), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,788 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 3.7 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,790 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 2.2 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,790 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.154.221:44581 (size: 2.2 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,791 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,791 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 10 (MapPartitionsRDD[64] at map at TensorflowRelation.scala:39) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,791 INFO cluster.YarnScheduler: Adding task set 10.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,806 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 13, algo-2, executor 1, partition 0, NODE_LOCAL, 7980 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,806 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 10.0 (TID 14, algo-2, executor 1, partition 1, NODE_LOCAL, 7980 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,830 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-2:35743 (size: 2.2 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:43,865 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-2:35743 (size: 42.3 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,497 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 10.0 (TID 14) in 691 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,518 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 13) in 722 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,518 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,518 INFO scheduler.DAGScheduler: ResultStage 10 (aggregate at TensorFlowInferSchema.scala:39) finished in 0.731 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,519 INFO scheduler.DAGScheduler: Job 10 finished: aggregate at TensorFlowInferSchema.scala:39, took 0.736597 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,571 INFO codegen.CodeGenerator: Code generated in 17.104736 ms\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,577 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,577 INFO scheduler.DAGScheduler: Got job 11 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,577 INFO scheduler.DAGScheduler: Final stage: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,577 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,578 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,578 INFO scheduler.DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[69] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,581 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 12.7 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,583 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 5.4 KB, free 363.8 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,583 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.154.221:44581 (size: 5.4 KB, free: 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,584 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,584 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[69] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,584 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,585 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 15, algo-2, executor 1, partition 0, NODE_LOCAL, 7980 bytes)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,594 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-2:35743 (size: 5.4 KB, free: 11.9 GB)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,682 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 15) in 97 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,683 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,683 INFO scheduler.DAGScheduler: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0) finished in 0.105 s\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,683 INFO scheduler.DAGScheduler: Job 11 finished: showString at NativeMethodAccessorImpl.java:0, took 0.106457 s\u001b[0m\n",
      "\u001b[34m+---------+--------------------+--------------------+--------------------+\u001b[0m\n",
      "\u001b[34m|label_ids|           input_ids|         segment_ids|          input_mask|\u001b[0m\n",
      "\u001b[34m+---------+--------------------+--------------------+--------------------+\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 102, 0...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 0, 0, 0...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 100, 1469, ...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 1019,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 2322,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1002, 3438,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1004, 1001,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1004, 1001,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1004, 1001,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        3|[101, 1004, 1001,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1004, 1001,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1004, 1001,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1006, 1037,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        3|[101, 1006, 2023,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        4|[101, 1008, 1008,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m|        0|[101, 1008, 3319,...|[0, 0, 0, 0, 0, 0...|[1, 1, 1, 1, 1, 1...|\u001b[0m\n",
      "\u001b[34m+---------+--------------------+--------------------+--------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,983 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,988 INFO server.AbstractConnector: Stopped Spark@64c5b64d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,990 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.154.221:4040\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:44,993 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,007 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,008 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,010 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,012 INFO cluster.YarnClientSchedulerBackend: Stopped\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,018 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,031 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,031 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,031 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,033 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,046 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,046 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,047 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-1dc54830-46da-4131-8d66-30cfd6f3780d\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,049 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-1dc54830-46da-4131-8d66-30cfd6f3780d/pyspark-5dd0497c-b15d-4591-bd07-6ef720b70741\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,051 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-b5c2f380-de95-44dc-930a-0a014db444ea\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,054 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,055 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\u001b[0m\n",
      "\u001b[34m2020-07-25 19:26:45,055 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m2020-07-25 19:26:46\u001b[0m\n",
      "\u001b[35mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[35mReceived end of job signal, exiting...\u001b[0m\n",
      "\u001b[34mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "running_processor.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Processed Output Dataset\n",
    "\n",
    "\n",
    "## _The next cells will not work properly until the job completes above._\n",
    "\n",
    "\n",
    "Take a look at a few rows of the transformed dataset to make sure the processing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $train_data_bert_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $validation_data_bert_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $test_data_bert_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
