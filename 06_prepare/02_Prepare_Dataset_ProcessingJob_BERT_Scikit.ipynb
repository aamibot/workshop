{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation with Amazon a SageMaker Processing Job and Scikit-Learn\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Scikit-Learn are used to pre-process data sets in order to prepare them for training. In this notebook we'll use Amazon SageMaker Processing, and leverage the power of Scikit-Learn in a managed SageMaker environment to run our processing workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/prepare_dataset_bert.png)\n",
    "\n",
    "![](img/processing.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. Setup Environment\n",
    "1. Setup Input Data\n",
    "1. Setup Output Data\n",
    "1. Build a Spark container for running the processing job\n",
    "1. Run the Processing Job using Amazon SageMaker\n",
    "1. Inspect the Processed Output Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment\n",
    "\n",
    "Let's start by specifying:\n",
    "* The S3 bucket and prefixes that you use for training and model data. Use the default bucket specified by the Amazon SageMaker session.\n",
    "* The IAM role ARN used to give processing and training access to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-140773038493/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "s3_raw_input_data = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(s3_raw_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-25 17:24:19   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\r\n",
      "2020-07-25 17:24:22   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_raw_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Processing Job using Amazon SageMaker\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the Spark container that was just built, and a SparkML script for processing in the job configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the Processing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from sklearn.model_selection import train_test_split\r\n",
      "from sklearn.utils import resample\r\n",
      "import functools\r\n",
      "import multiprocessing\r\n",
      "\r\n",
      "import pandas as pd\r\n",
      "from datetime import datetime\r\n",
      "import subprocess\r\n",
      "import sys\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\r\n",
      "import tensorflow as tf\r\n",
      "print(tf.__version__)\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\r\n",
      "from transformers import DistilBertTokenizer\r\n",
      "from tensorflow import keras\r\n",
      "import os\r\n",
      "import re\r\n",
      "import collections\r\n",
      "import argparse\r\n",
      "import json\r\n",
      "import os\r\n",
      "import pandas as pd\r\n",
      "import csv\r\n",
      "import glob\r\n",
      "from pathlib import Path\r\n",
      "\r\n",
      "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\r\n",
      "\r\n",
      "DATA_COLUMN = 'review_body'\r\n",
      "LABEL_COLUMN = 'star_rating'\r\n",
      "LABEL_VALUES = [1, 2, 3, 4, 5]\r\n",
      "    \r\n",
      "label_map = {}\r\n",
      "for (i, label) in enumerate(LABEL_VALUES):\r\n",
      "    label_map[label] = i\r\n",
      "\r\n",
      "    \r\n",
      "class InputFeatures(object):\r\n",
      "  \"\"\"BERT feature vectors.\"\"\"\r\n",
      "\r\n",
      "  def __init__(self,\r\n",
      "               input_ids,\r\n",
      "               input_mask,\r\n",
      "               segment_ids,\r\n",
      "               label_id):\r\n",
      "    self.input_ids = input_ids\r\n",
      "    self.input_mask = input_mask\r\n",
      "    self.segment_ids = segment_ids\r\n",
      "    self.label_id = label_id\r\n",
      "    \r\n",
      "    \r\n",
      "class Input(object):\r\n",
      "  \"\"\"A single training/test input for sequence classification.\"\"\"\r\n",
      "\r\n",
      "  def __init__(self, text, label=None):\r\n",
      "    \"\"\"Constructs an Input.\r\n",
      "    Args:\r\n",
      "      text: string. The untokenized text of the first sequence. For single\r\n",
      "        sequence tasks, only this sequence must be specified.\r\n",
      "      label: (Optional) string. The label of the example. This should be\r\n",
      "        specified for train and dev examples, but not for test examples.\r\n",
      "    \"\"\"\r\n",
      "    self.text = text\r\n",
      "    self.label = label\r\n",
      "    \r\n",
      "    \r\n",
      "def convert_input(text_input, max_seq_length):\r\n",
      "    # First, we need to preprocess our data so that it matches the data BERT was trained on:\r\n",
      "    #\r\n",
      "    # 1. Lowercase our text (if we're using a BERT lowercase model)\r\n",
      "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\r\n",
      "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\r\n",
      "    # \r\n",
      "    # Fortunately, the Transformers tokenizer does this for us!\r\n",
      "    #\r\n",
      "    tokens = tokenizer.tokenize(text_input.text)    \r\n",
      "\r\n",
      "    # Next, we need to do the following:\r\n",
      "    #\r\n",
      "    # 4. Map our words to indexes using a vocab file that BERT provides\r\n",
      "    # 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\r\n",
      "    # 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\r\n",
      "    #\r\n",
      "    # Again, the Transformers tokenizer does this for us!\r\n",
      "    #\r\n",
      "    encode_plus_tokens = tokenizer.encode_plus(text_input.text,\r\n",
      "                                               pad_to_max_length=True,\r\n",
      "                                               max_length=max_seq_length)\r\n",
      "\r\n",
      "    # Convert the text-based tokens to ids from the pre-trained BERT vocabulary\r\n",
      "    input_ids = encode_plus_tokens['input_ids']\r\n",
      "    # Specifies which tokens BERT should pay attention to (0 or 1)\r\n",
      "    input_mask = encode_plus_tokens['attention_mask']\r\n",
      "    # Segment Ids are always 0 for single-sequence tasks (or 1 if two-sequence tasks)\r\n",
      "    segment_ids = [0] * max_seq_length\r\n",
      "\r\n",
      "    # Label for our training data (star_rating 1 through 5)\r\n",
      "    label_id = label_map[text_input.label]\r\n",
      "\r\n",
      "    features = InputFeatures(\r\n",
      "        input_ids=input_ids,\r\n",
      "        input_mask=input_mask,\r\n",
      "        segment_ids=segment_ids,\r\n",
      "        label_id=label_id)\r\n",
      "\r\n",
      "#    print('**tokens**\\n{}\\n'.format(tokens))    \r\n",
      "#    print('**input_ids**\\n{}\\n'.format(features.input_ids))\r\n",
      "#    print('**input_mask**\\n{}\\n'.format(features.input_mask))\r\n",
      "#    print('**segment_ids**\\n{}\\n'.format(features.segment_ids))\r\n",
      "#    print('**label_id**\\n{}\\n'.format(features.label_id))\r\n",
      "\r\n",
      "    return features\r\n",
      "\r\n",
      "\r\n",
      "def convert_features_to_tfrecord(inputs,\r\n",
      "                                 output_file,\r\n",
      "                                 max_seq_length):\r\n",
      "    \"\"\"Convert a set of `Input`s to a TFRecord file.\"\"\"\r\n",
      "\r\n",
      "    tfrecord_writer = tf.io.TFRecordWriter(output_file)\r\n",
      "\r\n",
      "    for (input_idx, text_input) in enumerate(inputs):\r\n",
      "        if input_idx % 1000 == 0:\r\n",
      "            print(\"Writing example %d of %d\" % (input_idx, len(inputs)))\r\n",
      "\r\n",
      "            bert_features = convert_input(text_input, max_seq_length)\r\n",
      "        \r\n",
      "            tfrecord_features = collections.OrderedDict()\r\n",
      "            \r\n",
      "            tfrecord_features['input_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.input_ids))\r\n",
      "            tfrecord_features['input_mask'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.input_mask))\r\n",
      "            tfrecord_features['segment_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=bert_features.segment_ids))\r\n",
      "            tfrecord_features['label_ids'] = tf.train.Feature(int64_list=tf.train.Int64List(value=[bert_features.label_id]))\r\n",
      "\r\n",
      "            tfrecord = tf.train.Example(features=tf.train.Features(feature=tfrecord_features))\r\n",
      "            \r\n",
      "            tfrecord_writer.write(tfrecord.SerializeToString())\r\n",
      "\r\n",
      "    tfrecord_writer.close()\r\n",
      "    \r\n",
      "    \r\n",
      "def list_arg(raw_value):\r\n",
      "    \"\"\"argparse type for a list of strings\"\"\"\r\n",
      "    return str(raw_value).split(',')\r\n",
      "\r\n",
      "\r\n",
      "def parse_args():\r\n",
      "    # Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\r\n",
      "    resconfig = {}\r\n",
      "    try:\r\n",
      "        with open('/opt/ml/config/resourceconfig.json', 'r') as cfgfile:\r\n",
      "            resconfig = json.load(cfgfile)\r\n",
      "    except FileNotFoundError:\r\n",
      "        print('/opt/ml/config/resourceconfig.json not found.  current_host is unknown.')\r\n",
      "        pass # Ignore\r\n",
      "\r\n",
      "    # Local testing with CLI args\r\n",
      "    parser = argparse.ArgumentParser(description='Process')\r\n",
      "\r\n",
      "    parser.add_argument('--hosts', type=list_arg,\r\n",
      "        default=resconfig.get('hosts', ['unknown']),\r\n",
      "        help='Comma-separated list of host names running the job'\r\n",
      "    )\r\n",
      "    parser.add_argument('--current-host', type=str,\r\n",
      "        default=resconfig.get('current_host', 'unknown'),\r\n",
      "        help='Name of this host running the job'\r\n",
      "    )\r\n",
      "    parser.add_argument('--input-data', type=str,\r\n",
      "        default='/opt/ml/processing/input/data',\r\n",
      "    )\r\n",
      "    parser.add_argument('--output-data', type=str,\r\n",
      "        default='/opt/ml/processing/output',\r\n",
      "    )\r\n",
      "    parser.add_argument('--train-split-percentage', type=float,\r\n",
      "        default=0.90,\r\n",
      "    )\r\n",
      "    parser.add_argument('--validation-split-percentage', type=float,\r\n",
      "        default=0.05,\r\n",
      "    )    \r\n",
      "    parser.add_argument('--test-split-percentage', type=float,\r\n",
      "        default=0.05,\r\n",
      "    )\r\n",
      "    parser.add_argument('--balance-dataset', type=eval,\r\n",
      "        default=False\r\n",
      "    )\r\n",
      "    parser.add_argument('--max-seq-length', type=int,\r\n",
      "        default=128,\r\n",
      "    )  \r\n",
      "    \r\n",
      "    return parser.parse_args()\r\n",
      "\r\n",
      "    \r\n",
      "def _transform_tsv_to_tfrecord(file, \r\n",
      "                               max_seq_length, \r\n",
      "                               balance_dataset):\r\n",
      "    print('file {}'.format(file))\r\n",
      "    print('max_seq_length {}'.format(max_seq_length))\r\n",
      "    print('balance_dataset {}'.format(balance_dataset))\r\n",
      "\r\n",
      "    filename_without_extension = Path(Path(file).stem).stem\r\n",
      "\r\n",
      "    df = pd.read_csv(file, \r\n",
      "                     delimiter='\\t', \r\n",
      "                     quoting=csv.QUOTE_NONE,\r\n",
      "                     compression='gzip')\r\n",
      "\r\n",
      "    df.isna().values.any()\r\n",
      "    df = df.dropna()\r\n",
      "    df = df.reset_index(drop=True)\r\n",
      "\r\n",
      "    print('Shape of dataframe {}'.format(df.shape))\r\n",
      "\r\n",
      "    if balance_dataset:  \r\n",
      "        # Balance the dataset down to the minority class\r\n",
      "        from sklearn.utils import resample\r\n",
      "\r\n",
      "        five_star_df = df.query('star_rating == 5')\r\n",
      "        four_star_df = df.query('star_rating == 4')\r\n",
      "        three_star_df = df.query('star_rating == 3')\r\n",
      "        two_star_df = df.query('star_rating == 2')\r\n",
      "        one_star_df = df.query('star_rating == 1')\r\n",
      "\r\n",
      "        minority_count = min(five_star_df.shape[0], \r\n",
      "                             four_star_df.shape[0], \r\n",
      "                             three_star_df.shape[0], \r\n",
      "                             two_star_df.shape[0], \r\n",
      "                             one_star_df.shape[0]) \r\n",
      "\r\n",
      "        five_star_df = resample(five_star_df,\r\n",
      "                                replace = False,\r\n",
      "                                n_samples = minority_count,\r\n",
      "                                random_state = 27)\r\n",
      "\r\n",
      "        four_star_df = resample(four_star_df,\r\n",
      "                                replace = False,\r\n",
      "                                n_samples = minority_count,\r\n",
      "                                random_state = 27)\r\n",
      "\r\n",
      "        three_star_df = resample(three_star_df,\r\n",
      "                                 replace = False,\r\n",
      "                                 n_samples = minority_count,\r\n",
      "                                 random_state = 27)\r\n",
      "\r\n",
      "        two_star_df = resample(two_star_df,\r\n",
      "                               replace = False,\r\n",
      "                               n_samples = minority_count,\r\n",
      "                               random_state = 27)\r\n",
      "\r\n",
      "        one_star_df = resample(one_star_df,\r\n",
      "                               replace = False,\r\n",
      "                               n_samples = minority_count,\r\n",
      "                               random_state = 27)\r\n",
      "\r\n",
      "        df_balanced = pd.concat([five_star_df, four_star_df, three_star_df, two_star_df, one_star_df])\r\n",
      "        df_balanced = df_balanced.reset_index(drop=True)        \r\n",
      "        print('Shape of balanced dataframe {}'.format(df_balanced.shape))\r\n",
      "        df = df_balanced\r\n",
      "        \r\n",
      "    print('Shape of dataframe before splitting {}'.format(df.shape))\r\n",
      "    \r\n",
      "    print('train split percentage {}'.format(args.train_split_percentage))\r\n",
      "    print('validation split percentage {}'.format(args.validation_split_percentage))\r\n",
      "    print('test split percentage {}'.format(args.test_split_percentage))    \r\n",
      "    \r\n",
      "    holdout_percentage = 1.00 - args.train_split_percentage\r\n",
      "    print('holdout percentage {}'.format(holdout_percentage))\r\n",
      "    df_train, df_holdout = train_test_split(df, \r\n",
      "                                            test_size=holdout_percentage, \r\n",
      "                                            stratify=df['star_rating'])\r\n",
      "\r\n",
      "    test_holdout_percentage = args.test_split_percentage / holdout_percentage\r\n",
      "    print('test holdout percentage {}'.format(test_holdout_percentage))\r\n",
      "    df_validation, df_test = train_test_split(df_holdout, \r\n",
      "                                              test_size=test_holdout_percentage,\r\n",
      "                                              stratify=df_holdout['star_rating'])\r\n",
      "    \r\n",
      "    df_train = df_train.reset_index(drop=True)\r\n",
      "    df_validation = df_validation.reset_index(drop=True)\r\n",
      "    df_test = df_test.reset_index(drop=True)\r\n",
      "\r\n",
      "    print('Shape of train dataframe {}'.format(df_train.shape))\r\n",
      "    print('Shape of validation dataframe {}'.format(df_validation.shape))\r\n",
      "    print('Shape of test dataframe {}'.format(df_test.shape))\r\n",
      "\r\n",
      "    train_inputs = df_train.apply(lambda x: Input(text = x[DATA_COLUMN], \r\n",
      "                                                         label = x[LABEL_COLUMN]), axis = 1)\r\n",
      "\r\n",
      "    validation_inputs = df_validation.apply(lambda x: Input(text = x[DATA_COLUMN], \r\n",
      "                                                            label = x[LABEL_COLUMN]), axis = 1)\r\n",
      "\r\n",
      "    test_inputs = df_test.apply(lambda x: Input(text = x[DATA_COLUMN], \r\n",
      "                                                label = x[LABEL_COLUMN]), axis = 1)\r\n",
      "\r\n",
      "    # Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\r\n",
      "    # \r\n",
      "    # \r\n",
      "    # 1. Lowercase our text (if we're using a BERT lowercase model)\r\n",
      "    # 2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\r\n",
      "    # 3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\r\n",
      "    # 4. Map our words to indexes using a vocab file that BERT provides\r\n",
      "    # 5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\r\n",
      "    # 6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\r\n",
      "    # \r\n",
      "    # We don't have to worry about these details.  The Transformers tokenizer does this for us.\r\n",
      "    # \r\n",
      "    train_data = '{}/bert/train'.format(args.output_data)\r\n",
      "    validation_data = '{}/bert/validation'.format(args.output_data)\r\n",
      "    test_data = '{}/bert/test'.format(args.output_data)\r\n",
      "\r\n",
      "    # Convert our train and validation features to InputFeatures (.tfrecord protobuf) that works with BERT and TensorFlow.\r\n",
      "    df_train_embeddings = convert_features_to_tfrecord(train_inputs, \r\n",
      "                                                       '{}/part-{}-{}.tfrecord'.format(train_data, args.current_host, filename_without_extension), \r\n",
      "                                                       max_seq_length)\r\n",
      "\r\n",
      "    df_validation_embeddings = convert_features_to_tfrecord(validation_inputs, '{}/part-{}-{}.tfrecord'.format(validation_data, args.current_host, filename_without_extension), max_seq_length)\r\n",
      "\r\n",
      "    df_test_embeddings = convert_features_to_tfrecord(test_inputs, '{}/part-{}-{}.tfrecord'.format(test_data, args.current_host, filename_without_extension), max_seq_length)\r\n",
      "        \r\n",
      "    \r\n",
      "def process(args):\r\n",
      "    print('Current host: {}'.format(args.current_host))\r\n",
      "    \r\n",
      "    train_data = None\r\n",
      "    validation_data = None\r\n",
      "    test_data = None\r\n",
      "\r\n",
      "    transform_tsv_to_tfrecord = functools.partial(_transform_tsv_to_tfrecord, \r\n",
      "                                                 max_seq_length=args.max_seq_length,\r\n",
      "                                                 balance_dataset=args.balance_dataset\r\n",
      "\r\n",
      "    )\r\n",
      "    input_files = glob.glob('{}/*.tsv.gz'.format(args.input_data))\r\n",
      "\r\n",
      "    num_cpus = multiprocessing.cpu_count()\r\n",
      "    print('num_cpus {}'.format(num_cpus))\r\n",
      "\r\n",
      "    p = multiprocessing.Pool(num_cpus)\r\n",
      "    p.map(transform_tsv_to_tfrecord, input_files)\r\n",
      "\r\n",
      "    print('Listing contents of {}'.format(args.output_data))\r\n",
      "    dirs_output = os.listdir(args.output_data)\r\n",
      "    for file in dirs_output:\r\n",
      "        print(file)\r\n",
      "\r\n",
      "    print('Listing contents of {}'.format(train_data))\r\n",
      "    dirs_output = os.listdir(train_data)\r\n",
      "    for file in dirs_output:\r\n",
      "        print(file)\r\n",
      "\r\n",
      "    print('Listing contents of {}'.format(validation_data))\r\n",
      "    dirs_output = os.listdir(validation_data)\r\n",
      "    for file in dirs_output:\r\n",
      "        print(file)\r\n",
      "\r\n",
      "    print('Listing contents of {}'.format(test_data))\r\n",
      "    dirs_output = os.listdir(test_data)\r\n",
      "    for file in dirs_output:\r\n",
      "        print(file)\r\n",
      "\r\n",
      "    print('Complete')\r\n",
      "    \r\n",
      "    \r\n",
      "if __name__ == \"__main__\":\r\n",
      "    args = parse_args()\r\n",
      "    print('Loaded arguments:')\r\n",
      "    print(args)\r\n",
      "    \r\n",
      "    print('Environment variables:')\r\n",
      "    print(os.environ)\r\n",
      "\r\n",
      "    process(args)    \r\n"
     ]
    }
   ],
   "source": [
    "!cat preprocess-scikit-text-to-bert.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this script as a processing job.  You also need to specify one `ProcessingInput` with the `source` argument of the Amazon S3 bucket and `destination` is where the script reads this data from `/opt/ml/processing/input` (inside the Docker container.)  All local paths inside the processing container must begin with `/opt/ml/processing/`.\n",
    "\n",
    "Also give the `run()` method a `ProcessingOutput`, where the `source` is the path the script writes output data to.  For outputs, the `destination` defaults to an S3 bucket that the Amazon SageMaker Python SDK creates for you, following the format `s3://sagemaker-<region>-<account_id>/<processing_job_name>/output/<output_name>/`.  You also give the `ProcessingOutput` value for `output_name`, to make it easier to retrieve these output artifacts after the job is run.\n",
    "\n",
    "The arguments parameter in the `run()` method are command-line arguments in our `preprocess-scikit-text-to-bert.py` script.\n",
    "\n",
    "Note that we sharding the data using `ShardedS3Key` to spread the transformations across all worker nodes in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                             role=role,\n",
    "                             instance_type='ml.c5.2xlarge',\n",
    "                             instance_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Train, Validation, Split Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_percentage = 0.90\n",
    "validation_split_percentage = 0.05\n",
    "test_split_percentage = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Maximum Sequence Length for the BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_dataset = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2020-07-25-18-35-13-587\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/amazon-reviews-pds/tsv/', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/sagemaker-scikit-learn-2020-07-25-18-35-13-587/input/code/preprocess-scikit-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/sagemaker-scikit-learn-2020-07-25-18-35-13-587/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/sagemaker-scikit-learn-2020-07-25-18-35-13-587/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/sagemaker-scikit-learn-2020-07-25-18-35-13-587/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "processor.run(code='preprocess-scikit-text-to-bert.py',\n",
    "              inputs=[ProcessingInput(source=s3_raw_input_data,\n",
    "                                      destination='/opt/ml/processing/input/data/',\n",
    "                                      s3_data_distribution_type='ShardedByS3Key')\n",
    "              ],\n",
    "              outputs=[\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-train',\n",
    "                                        source='/opt/ml/processing/output/bert/train'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-validation',\n",
    "                                        source='/opt/ml/processing/output/bert/validation'),\n",
    "                       ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                        output_name='bert-test',\n",
    "                                        source='/opt/ml/processing/output/bert/test'),\n",
    "              ],\n",
    "              arguments=['--train-split-percentage', str(train_split_percentage),\n",
    "                         '--validation-split-percentage', str(validation_split_percentage),\n",
    "                         '--test-split-percentage', str(test_split_percentage),\n",
    "                         '--max-seq-length', str(max_seq_length),\n",
    "                         '--balance-dataset', str(balance_dataset)\n",
    "              ],\n",
    "              logs=True,\n",
    "              wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-scikit-learn-2020-07-25-18-35-13-587\n"
     ]
    }
   ],
   "source": [
    "scikit_processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "print(scikit_processing_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-west-2#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=sagemaker-scikit-learn-2020-07-25-18-35-13-587;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, scikit_processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-west-2-140773038493/sagemaker-scikit-learn-2020-07-25-18-35-13-587/?region=us-west-2&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Processing Job Has Completed</b>'.format(bucket, scikit_processing_job_name, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List Processing Jobs through boto3 Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ProcessingJobSummaries': [{'ProcessingJobName': 'sagemaker-scikit-learn-2020-07-25-18-35-13-587',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:140773038493:processing-job/sagemaker-scikit-learn-2020-07-25-18-35-13-587',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 25, 18, 35, 14, 34000, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 25, 18, 35, 14, 34000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'InProgress'},\n",
       "  {'ProcessingJobName': 'spark-amazon-reviews-analyzer-2020-07-25-18-24-54-247',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:140773038493:processing-job/spark-amazon-reviews-analyzer-2020-07-25-18-24-54-247',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 25, 18, 24, 54, 670000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 7, 25, 18, 31, 3, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 25, 18, 31, 3, 845000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'pr-1-c4f44ecca3fb4f218b58302e584c68f02d460f75f7f04dd68d369ab9ab',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:140773038493:processing-job/pr-1-c4f44ecca3fb4f218b58302e584c68f02d460f75f7f04dd68d369ab9ab',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 25, 16, 20, 31, 260000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 7, 25, 16, 25, 11, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 25, 16, 25, 11, 834000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'},\n",
       "  {'ProcessingJobName': 'db-1-f0c237a12a5c4634bbd1f16b082fd142610fe48573454ed98d3d2268d7',\n",
       "   'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:140773038493:processing-job/db-1-f0c237a12a5c4634bbd1f16b082fd142610fe48573454ed98d3d2268d7',\n",
       "   'CreationTime': datetime.datetime(2020, 7, 25, 16, 16, 44, 9000, tzinfo=tzlocal()),\n",
       "   'ProcessingEndTime': datetime.datetime(2020, 7, 25, 16, 20, 12, tzinfo=tzlocal()),\n",
       "   'LastModifiedTime': datetime.datetime(2020, 7, 25, 16, 20, 12, 65000, tzinfo=tzlocal()),\n",
       "   'ProcessingJobStatus': 'Completed'}],\n",
       " 'ResponseMetadata': {'RequestId': '28fb5d30-2958-48cc-b8fb-648d94fa6849',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '28fb5d30-2958-48cc-b8fb-648d94fa6849',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1399',\n",
       "   'date': 'Sat, 25 Jul 2020 18:35:14 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.list_processing_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Wait Until the Processing Job Completes\n",
    "Re-run this next cell until the job status shows `Completed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'ProcessingInputs': [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/amazon-reviews-pds/tsv/', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/sagemaker-scikit-learn-2020-07-25-18-35-13-587/input/code/preprocess-scikit-text-to-bert.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'bert-train', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/sagemaker-scikit-learn-2020-07-25-18-35-13-587/output/bert-train', 'LocalPath': '/opt/ml/processing/output/bert/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-validation', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/sagemaker-scikit-learn-2020-07-25-18-35-13-587/output/bert-validation', 'LocalPath': '/opt/ml/processing/output/bert/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'bert-test', 'S3Output': {'S3Uri': 's3://sagemaker-us-west-2-140773038493/sagemaker-scikit-learn-2020-07-25-18-35-13-587/output/bert-test', 'LocalPath': '/opt/ml/processing/output/bert/test', 'S3UploadMode': 'EndOfJob'}}]}, 'ProcessingJobName': 'sagemaker-scikit-learn-2020-07-25-18-35-13-587', 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.c5.2xlarge', 'VolumeSizeInGB': 30}}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'AppSpecification': {'ImageUri': '246618743249.dkr.ecr.us-west-2.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3', 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/preprocess-scikit-text-to-bert.py'], 'ContainerArguments': ['--train-split-percentage', '0.9', '--validation-split-percentage', '0.05', '--test-split-percentage', '0.05', '--max-seq-length', '128', '--balance-dataset', 'False']}, 'RoleArn': 'arn:aws:iam::140773038493:role/TeamRole', 'ProcessingJobArn': 'arn:aws:sagemaker:us-west-2:140773038493:processing-job/sagemaker-scikit-learn-2020-07-25-18-35-13-587', 'ProcessingJobStatus': 'InProgress', 'LastModifiedTime': datetime.datetime(2020, 7, 25, 18, 35, 14, 34000, tzinfo=tzlocal()), 'CreationTime': datetime.datetime(2020, 7, 25, 18, 35, 14, 34000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '0546d275-0337-4e85-88d1-8fba9474a5b6', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '0546d275-0337-4e85-88d1-8fba9474a5b6', 'content-type': 'application/x-amz-json-1.1', 'content-length': '2272', 'date': 'Sat, 25 Jul 2020 18:35:13 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=scikit_processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n",
    "processing_job_description = running_processor.describe()\n",
    "\n",
    "processing_job_status = processing_job_description['ProcessingJobStatus']\n",
    "print('\\n')\n",
    "print(processing_job_status)\n",
    "print('\\n')\n",
    "\n",
    "print(processing_job_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Please Wait Until the ^^ Processing Job ^^ Completes Above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................!"
     ]
    }
   ],
   "source": [
    "running_processor.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Processed Output Data\n",
    "\n",
    "Take a look at a few rows of the transformed dataset to make sure the processing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-140773038493/sagemaker-scikit-learn-2020-07-25-18-35-13-587/output/bert-train\n",
      "s3://sagemaker-us-west-2-140773038493/sagemaker-scikit-learn-2020-07-25-18-35-13-587/output/bert-validation\n",
      "s3://sagemaker-us-west-2-140773038493/sagemaker-scikit-learn-2020-07-25-18-35-13-587/output/bert-test\n"
     ]
    }
   ],
   "source": [
    "output_config = processing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'bert-train':\n",
    "        processed_train_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-validation':\n",
    "        processed_validation_data_s3_uri = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'bert-test':\n",
    "        processed_test_data_s3_uri = output['S3Output']['S3Uri']\n",
    "        \n",
    "print(processed_train_data_s3_uri)\n",
    "print(processed_validation_data_s3_uri)\n",
    "print(processed_test_data_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-25 18:39:19      49691 part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\r\n",
      "2020-07-25 18:39:24      71632 part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_train_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-25 18:39:19       3596 part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\r\n",
      "2020-07-25 18:39:24       4570 part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_validation_data_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-25 18:39:19       3569 part-algo-1-amazon_reviews_us_Digital_Software_v1_00.tfrecord\r\n",
      "2020-07-25 18:39:24       4582 part-algo-2-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $processed_test_data_s3_uri/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass Variables to the Next Notebook(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 's3_raw_input_data' (str)\n"
     ]
    }
   ],
   "source": [
    "%store s3_raw_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'max_seq_length' (int)\n"
     ]
    }
   ],
   "source": [
    "%store max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_split_percentage' (float)\n"
     ]
    }
   ],
   "source": [
    "%store train_split_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'validation_split_percentage' (float)\n"
     ]
    }
   ],
   "source": [
    "%store validation_split_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'test_split_percentage' (float)\n"
     ]
    }
   ],
   "source": [
    "%store test_split_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'balance_dataset' (bool)\n"
     ]
    }
   ],
   "source": [
    "%store balance_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'processed_train_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store processed_train_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'processed_validation_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store processed_validation_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'processed_test_data_s3_uri' (str)\n"
     ]
    }
   ],
   "source": [
    "%store processed_test_data_s3_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "autopilot_endpoint_name                      -> 'automl-dm-ep-25-16-51-33'\n",
      "balance_dataset                              -> False\n",
      "comprehend_endpoint_arn                      -> 'arn:aws:comprehend:us-west-2:140773038493:documen\n",
      "header_train_s3_uri                          -> 's3://sagemaker-us-west-2-140773038493/data/amazon\n",
      "max_seq_length                               -> 128\n",
      "noheader_train_s3_uri                        -> 's3://sagemaker-us-west-2-140773038493/data/amazon\n",
      "processed_test_data_s3_uri                   -> 's3://sagemaker-us-west-2-140773038493/sagemaker-s\n",
      "processed_train_data_s3_uri                  -> 's3://sagemaker-us-west-2-140773038493/sagemaker-s\n",
      "processed_validation_data_s3_uri             -> 's3://sagemaker-us-west-2-140773038493/sagemaker-s\n",
      "s3_raw_input_data                            -> 's3://sagemaker-us-west-2-140773038493/amazon-revi\n",
      "test_split_percentage                        -> 0.05\n",
      "train_split_percentage                       -> 0.9\n",
      "validation_split_percentage                  -> 0.05\n"
     ]
    }
   ],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
